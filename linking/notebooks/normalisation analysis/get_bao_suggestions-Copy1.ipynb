{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e3711a-a8c9-480d-8149-005bc34b1566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-13 13:11:41.882338: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-13 13:11:46.133804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-13 13:11:47.713632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-13 13:11:48.111676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-13 13:11:50.558870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 13:12:11.155210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pronto import Ontology\n",
    "import spacy\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import re\n",
    "import pandas\n",
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad8182-4f14-4ba9-be65-1e3a975e645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "\n",
    "# santosh path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/source_data/knowledge_base/bao/BAO.csv\"\n",
    "path__ = \"/homes/ines\"\n",
    "INPUT_FILENAME = path__ + \"/dictionary/BAO.csv\"\n",
    "OUTPUT_PICKLE_FILENAME = path__ + \"/dictionary/bao.pkl\"\n",
    "OUTPUT_LIST = path__ + \"/dictionary/bao_list.txt\"\n",
    "FAISS_INDEX_FILENAME = path__ + \"/dictionary/bao_terms.index\"\n",
    "# OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adb79fc-08b7-49c3-b1d6-451609308921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████| 564/564 [00:00<00:00, 917kB/s]\n",
      "vocab.txt: 100%|███████████████████████████| 232k/232k [00:00<00:00, 1.58MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 712k/712k [00:00<00:00, 2.41MB/s]\n",
      "special_tokens_map.json: 100%|████████████████| 125/125 [00:00<00:00, 254kB/s]\n",
      "config.json: 100%|███████████████████████████| 720/720 [00:00<00:00, 1.50MB/s]\n",
      "pytorch_model.bin: 100%|█████████████████| 90.9M/90.9M [00:02<00:00, 35.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-2.7119e-02,  9.8441e-02, -4.8616e-02,  1.2813e-01, -3.2804e-02,\n",
      "          2.5970e-02,  1.5071e-01,  5.6532e-02, -1.4452e-02, -1.0685e-02,\n",
      "         -5.9629e-02,  6.7335e-02, -7.7009e-02, -2.6510e-02, -8.2213e-02,\n",
      "         -7.1197e-02, -9.9482e-03, -3.2808e-02, -4.1267e-03,  1.3900e-02,\n",
      "          7.8650e-02,  6.7884e-02,  7.2451e-02,  3.8066e-02,  2.2594e-03,\n",
      "          5.1958e-03,  6.6761e-02, -5.6698e-02, -5.5391e-02, -5.5787e-02,\n",
      "         -4.6120e-02,  3.8803e-02,  9.4903e-03,  6.7905e-03, -7.3449e-02,\n",
      "          8.6693e-02,  8.5998e-03, -1.2169e-02, -8.1756e-02, -6.2011e-02,\n",
      "          4.1464e-02, -5.4746e-02, -1.2144e-02,  7.2727e-02,  4.6662e-02,\n",
      "         -6.0085e-03, -7.6413e-02,  5.9721e-02,  5.6087e-02,  8.2868e-02,\n",
      "         -5.2568e-02, -2.4164e-02,  3.5872e-02, -5.4179e-03,  9.6168e-02,\n",
      "         -5.0484e-02, -9.6207e-02, -5.9018e-02, -5.0531e-02, -1.0543e-01,\n",
      "         -2.1302e-02,  2.0079e-02, -3.4305e-03,  2.3637e-02,  6.8764e-02,\n",
      "         -5.5543e-02, -3.5659e-02,  1.9404e-02,  8.6421e-03, -1.4179e-02,\n",
      "         -4.7018e-02, -4.9330e-02, -1.6548e-02,  4.8497e-02,  3.3775e-02,\n",
      "         -2.3433e-02, -3.9166e-02, -5.6307e-02,  1.7761e-02, -1.1311e-02,\n",
      "          7.1016e-02,  1.2001e-02,  6.7637e-02,  6.7640e-02,  2.6665e-02,\n",
      "         -1.1223e-02,  1.0118e-01, -1.7141e-02, -7.5236e-02, -4.4499e-02,\n",
      "         -8.0261e-02, -4.4453e-02, -2.8425e-02,  4.1454e-02, -6.9278e-02,\n",
      "          1.3767e-02, -2.2871e-03, -1.6787e-01, -7.8148e-03,  2.0105e-01,\n",
      "          1.6358e-02,  1.1781e-02,  7.3621e-02,  6.8137e-02, -2.6521e-02,\n",
      "          1.9602e-02,  2.6559e-02,  2.9855e-02,  2.7818e-02,  2.9387e-02,\n",
      "         -2.5109e-02,  3.4435e-02,  4.5484e-02,  2.4380e-02, -8.5691e-02,\n",
      "          2.5873e-02, -7.0816e-03, -5.6700e-02, -1.1988e-02,  5.0334e-02,\n",
      "         -6.6808e-02, -1.6543e-02,  3.4780e-02, -5.2860e-02, -2.4131e-02,\n",
      "          4.5475e-02,  3.0206e-02,  3.4734e-07,  6.1422e-02, -1.1997e-01,\n",
      "          1.7652e-02,  2.1382e-02,  4.3064e-02,  6.0673e-02, -1.5988e-02,\n",
      "          2.6240e-02,  3.8798e-02,  4.0830e-04, -1.7459e-02, -9.3192e-02,\n",
      "          1.2742e-02,  2.6110e-02,  4.3545e-02, -2.3000e-02,  5.0960e-03,\n",
      "         -3.6863e-03,  8.8516e-02,  4.2965e-02, -6.2063e-02,  4.7765e-02,\n",
      "          3.3605e-02,  3.1659e-02, -5.7600e-02, -9.9926e-03, -3.3138e-02,\n",
      "          7.3674e-03,  6.0373e-02,  2.2884e-02,  1.3623e-02,  6.9463e-02,\n",
      "          5.2599e-02, -6.0657e-02,  3.0186e-02, -1.6978e-02,  1.4825e-02,\n",
      "         -1.0041e-02,  1.4252e-02, -7.7614e-03,  2.2563e-02, -1.7426e-02,\n",
      "          4.3921e-02, -5.9640e-02,  2.4045e-02,  9.7767e-03,  2.0543e-02,\n",
      "         -4.7662e-02, -7.3722e-02,  5.2892e-02, -3.2059e-02, -7.2182e-03,\n",
      "          2.8419e-02,  6.6378e-02,  3.3796e-02, -1.6826e-02, -2.2320e-02,\n",
      "         -1.0903e-02, -1.7943e-02, -2.2420e-03, -2.1489e-02,  9.9740e-02,\n",
      "         -4.1298e-02, -6.0786e-04, -3.4294e-02, -2.0792e-02, -3.0027e-02,\n",
      "         -5.1564e-02, -7.6396e-02, -6.3556e-02, -1.2373e-02, -4.8622e-02,\n",
      "          4.8820e-02, -3.4862e-02, -7.2608e-02,  1.9211e-02,  6.5171e-03,\n",
      "         -2.5152e-02, -5.1529e-02, -6.9091e-02,  2.3861e-03, -9.5659e-03,\n",
      "          5.8601e-02, -2.8724e-03, -2.9251e-02,  1.0437e-01, -6.3688e-02,\n",
      "         -1.4445e-02, -1.2782e-02, -5.6571e-02, -1.1913e-01,  4.2745e-02,\n",
      "         -2.6207e-02, -4.1455e-02,  2.5840e-02,  1.9374e-06, -9.5289e-03,\n",
      "         -4.6297e-02,  2.0257e-02,  1.6913e-02,  8.2977e-02, -4.9380e-02,\n",
      "         -1.2040e-02, -8.1100e-02,  2.7762e-02,  2.1085e-02,  2.1716e-02,\n",
      "          2.6711e-02,  5.1675e-02, -3.9209e-02, -3.9897e-04,  7.1761e-02,\n",
      "         -6.5745e-02,  6.4219e-02, -2.3587e-02, -1.7605e-02, -2.4535e-02,\n",
      "          6.0555e-02,  2.9494e-03, -4.5098e-02,  2.9133e-02,  6.8570e-02,\n",
      "         -1.5080e-02,  4.0447e-02, -9.0276e-02, -8.0717e-03,  5.6683e-02,\n",
      "         -3.4325e-02, -4.9521e-02, -2.4682e-02, -3.3434e-03, -1.0326e-02,\n",
      "         -7.2007e-02, -2.9917e-02, -5.7263e-02, -1.9944e-02,  1.0055e-01,\n",
      "         -9.6635e-03,  4.4679e-02,  2.6437e-02,  1.4652e-01, -1.2559e-02,\n",
      "         -5.5769e-02,  3.3679e-02,  4.6291e-02, -4.8265e-03, -5.4304e-02,\n",
      "          4.0944e-03, -6.8711e-03,  5.8073e-02, -2.7583e-02, -4.4286e-02,\n",
      "         -9.2978e-03, -3.0877e-02,  2.6926e-02,  2.1596e-02,  3.8388e-03,\n",
      "         -2.2027e-02,  1.0910e-04,  1.1502e-02,  3.4573e-03, -1.1643e-02,\n",
      "          6.3833e-02, -1.1995e-02,  9.5440e-02,  9.9509e-02,  5.5653e-02,\n",
      "         -6.9397e-03, -9.3163e-02,  5.4413e-02, -2.2148e-02,  3.3488e-02,\n",
      "         -8.1751e-02,  9.3176e-02, -3.2770e-02, -6.8385e-02, -4.2128e-02,\n",
      "         -3.7395e-02, -1.4236e-02, -5.0421e-02, -8.8686e-02, -5.5774e-02,\n",
      "          2.3463e-02, -5.3649e-02, -2.9961e-03,  1.5989e-02, -6.2554e-02,\n",
      "         -1.3121e-02, -9.0776e-02, -4.6379e-02, -2.6999e-02,  2.1782e-06,\n",
      "          9.4676e-02, -3.3740e-02, -2.5076e-02,  1.1721e-02, -6.5118e-03,\n",
      "          1.3396e-02,  9.8507e-03,  8.7897e-02,  9.9964e-02, -4.1784e-03,\n",
      "         -6.7993e-03,  1.4331e-01, -1.1440e-02,  4.6921e-03,  3.4958e-02,\n",
      "         -7.3630e-03, -5.2655e-02,  1.0072e-01, -3.8154e-02,  1.5928e-02,\n",
      "         -4.3692e-02, -3.1276e-02, -2.3574e-03,  2.0063e-02, -5.2306e-02,\n",
      "         -6.6999e-02,  6.1564e-02, -3.7612e-02, -1.1605e-02, -7.3265e-02,\n",
      "          1.1210e-02,  8.1493e-02,  7.2013e-02,  7.7729e-02, -5.0805e-02,\n",
      "         -1.3582e-02,  6.1483e-02,  1.8792e-02, -3.5406e-02, -7.0749e-02,\n",
      "          1.6161e-02,  2.0129e-02,  1.0390e-02, -1.7817e-02, -5.3351e-02,\n",
      "         -3.3550e-02,  9.0182e-02,  3.4840e-02,  6.0584e-02,  7.0346e-03,\n",
      "         -7.5342e-02,  8.1303e-02, -3.5841e-02, -2.9838e-03,  2.6616e-02,\n",
      "          7.8819e-03, -8.4647e-02, -7.5232e-03, -2.0726e-02, -4.8930e-02,\n",
      "          8.2707e-02,  1.3677e-02,  3.9125e-02,  2.4707e-03],\n",
      "        [-2.1434e-03, -2.6362e-02,  2.0756e-02,  5.7278e-02, -6.8206e-02,\n",
      "         -3.7082e-02,  7.5204e-02,  7.6173e-02, -2.6864e-02, -1.6652e-02,\n",
      "          2.5511e-02,  6.5178e-03, -8.0748e-02, -9.0136e-03, -2.5223e-02,\n",
      "          2.1891e-02,  1.7062e-02, -9.4377e-02,  3.5048e-02, -4.0325e-03,\n",
      "          6.0794e-02, -3.0680e-02,  4.8326e-02,  1.1614e-02, -4.1483e-02,\n",
      "         -1.3214e-02, -4.2557e-02,  5.1162e-02,  5.7535e-02, -1.3510e-01,\n",
      "         -8.2804e-02,  7.7640e-02,  3.9618e-03,  4.3203e-02, -2.9818e-03,\n",
      "         -8.6820e-02,  2.8907e-02,  1.0358e-01, -1.4244e-02,  5.6562e-03,\n",
      "          1.5323e-02, -5.1552e-02,  7.6237e-02, -2.2918e-02,  2.7365e-02,\n",
      "         -5.1611e-02,  2.7695e-02, -4.3177e-02, -4.8097e-02, -9.4084e-03,\n",
      "         -2.8367e-02, -4.0242e-04, -4.6083e-02,  6.2117e-02,  2.8387e-02,\n",
      "          2.1143e-02, -1.1264e-01,  9.2398e-02,  1.5671e-03,  3.2463e-02,\n",
      "          5.8176e-02,  6.0789e-03, -5.4128e-02, -7.0706e-02,  8.5312e-02,\n",
      "          3.1215e-02, -1.4045e-02, -6.7534e-02,  5.1422e-03,  3.5236e-02,\n",
      "         -1.9185e-02,  1.0084e-02, -9.1916e-02, -1.1663e-02,  6.3272e-02,\n",
      "         -7.5630e-02,  3.2596e-02, -4.4255e-02,  2.4276e-02, -9.7103e-03,\n",
      "         -1.0050e-01, -5.7057e-03, -1.4877e-02,  5.2418e-02, -3.6921e-02,\n",
      "         -2.4000e-02, -1.5545e-02, -8.8015e-04, -8.1232e-03,  3.0917e-02,\n",
      "          1.5975e-02,  1.7307e-02,  8.9496e-02,  2.1094e-02, -3.7538e-02,\n",
      "          3.9016e-02,  3.4943e-02, -1.8962e-02,  2.8673e-02,  2.0101e-01,\n",
      "          1.5518e-02, -5.8545e-03, -1.5178e-03, -1.4573e-02,  5.5739e-02,\n",
      "         -5.8535e-02,  5.3644e-03, -3.5248e-02,  3.2469e-03, -5.0569e-02,\n",
      "         -3.3210e-02,  4.3275e-03, -8.4109e-02, -2.8683e-02, -6.2475e-02,\n",
      "          6.3914e-02, -4.5353e-03,  6.8708e-02,  7.6704e-02,  2.2601e-03,\n",
      "          5.7522e-02, -4.3275e-02,  8.8739e-03, -9.4580e-02, -6.5227e-02,\n",
      "         -7.4082e-02, -3.9345e-02, -1.7277e-06,  7.3522e-03,  6.9454e-02,\n",
      "         -3.7494e-03,  1.3311e-01,  4.0362e-02,  7.1773e-03, -5.0124e-02,\n",
      "         -2.6019e-02,  2.9937e-02, -4.6734e-03, -4.2259e-02, -3.6518e-02,\n",
      "         -5.8329e-02,  6.2686e-02,  1.1265e-01, -3.8792e-02, -6.7869e-04,\n",
      "          5.3165e-02, -9.7582e-02, -4.1115e-02,  1.7406e-02,  9.2725e-02,\n",
      "          1.7642e-02,  2.7782e-02,  7.6506e-02,  5.2337e-02,  4.2703e-02,\n",
      "         -5.6690e-02,  3.2697e-02,  4.4198e-02, -1.8958e-03, -6.0359e-02,\n",
      "         -8.4166e-02, -3.0376e-02, -1.7128e-02, -3.8593e-02, -5.3815e-03,\n",
      "         -5.2491e-02,  6.3316e-02, -5.8360e-02,  2.7855e-02,  7.6213e-02,\n",
      "          1.1819e-02,  1.5713e-02,  7.8312e-02, -1.5305e-02,  1.3157e-01,\n",
      "          5.6083e-02, -5.0573e-02,  2.3932e-03, -4.5426e-02, -8.9708e-03,\n",
      "         -1.5878e-02,  1.7769e-02,  1.2459e-03, -2.2444e-03,  8.8599e-02,\n",
      "          6.0647e-02,  4.1475e-02,  2.1742e-02,  7.1737e-02,  5.2131e-02,\n",
      "          4.0740e-02, -9.7445e-02, -8.2607e-02, -1.0469e-02, -8.6678e-02,\n",
      "         -7.8157e-02,  2.9475e-02, -3.7624e-02,  4.7299e-02,  7.2133e-02,\n",
      "          1.9663e-02,  5.5660e-02, -4.2625e-03, -3.5334e-02,  4.4379e-03,\n",
      "         -2.7695e-02,  1.5095e-02, -4.5068e-02, -1.0779e-01, -2.3188e-02,\n",
      "         -1.2617e-03,  6.6744e-02, -2.5691e-02, -4.1707e-02, -3.5542e-02,\n",
      "         -6.8120e-02,  6.3739e-02,  4.7709e-02, -1.2670e-01,  2.1957e-02,\n",
      "         -1.1276e-02, -4.2616e-02,  9.8252e-03,  2.3090e-06, -9.5413e-02,\n",
      "          4.7441e-02, -4.3515e-02,  1.7207e-02, -2.3122e-02, -1.0968e-02,\n",
      "          7.2081e-03, -2.9592e-03, -3.6451e-03,  7.2986e-02,  1.0149e-01,\n",
      "          1.4093e-02,  1.1409e-01, -2.8296e-02,  2.2785e-02, -1.9278e-02,\n",
      "         -4.6552e-02,  1.0315e-02,  1.9252e-02,  1.2441e-02,  1.6094e-02,\n",
      "         -2.0430e-02,  1.2443e-02,  9.3841e-02, -2.7984e-02,  8.4765e-02,\n",
      "          2.0977e-02,  4.2541e-02, -4.7102e-02, -2.6147e-02, -3.8862e-02,\n",
      "          2.6417e-03, -1.1757e-02,  3.6017e-02, -3.4860e-02, -3.9298e-03,\n",
      "          5.1398e-02,  3.4112e-02, -6.9244e-02,  6.1386e-02,  3.2675e-02,\n",
      "         -6.2485e-03,  8.0765e-02,  1.3303e-01, -3.6642e-02, -9.8652e-03,\n",
      "          1.2282e-01,  1.2791e-02,  5.8708e-02,  2.0464e-02, -5.8901e-02,\n",
      "         -2.1322e-02, -2.5475e-02,  3.7572e-02, -2.8290e-02, -6.0203e-02,\n",
      "         -2.2827e-02,  4.9056e-04, -5.7464e-02,  4.0374e-02,  1.1435e-02,\n",
      "         -1.9649e-02, -7.1798e-03,  6.0294e-02,  1.0803e-02, -1.2662e-01,\n",
      "         -6.2931e-02,  1.3040e-02, -8.4374e-02,  5.0777e-02,  9.8909e-03,\n",
      "          8.9253e-03,  2.5605e-02, -5.6390e-02, -7.5910e-02, -7.4668e-02,\n",
      "         -1.4134e-03,  6.2589e-02,  5.5477e-02, -1.6358e-02, -6.0813e-02,\n",
      "          1.6159e-02,  6.9759e-02,  9.6403e-03,  8.9483e-03, -1.2896e-02,\n",
      "          6.3514e-02,  5.6869e-02, -1.2424e-02, -1.7092e-03, -2.1921e-02,\n",
      "          3.5182e-02, -2.5340e-02,  4.4121e-02, -5.8859e-02,  2.3135e-06,\n",
      "          1.9330e-03, -2.9640e-02,  6.5424e-02, -3.0152e-02, -1.9034e-02,\n",
      "         -5.5035e-03, -1.6455e-01,  7.2097e-02,  1.1298e-02,  2.0276e-02,\n",
      "         -9.6381e-02,  2.5452e-02,  6.8119e-02,  5.9906e-03,  1.1615e-01,\n",
      "          6.7604e-03,  3.9922e-02,  1.0966e-02, -9.0318e-02, -4.1849e-02,\n",
      "         -4.0502e-03, -7.8456e-02, -1.0431e-03, -2.1475e-02, -3.4250e-02,\n",
      "          4.0181e-02, -1.9490e-02,  2.5739e-02,  8.0549e-04,  4.2523e-02,\n",
      "          8.5442e-03,  9.6660e-02, -7.4613e-03, -5.3160e-02, -7.8248e-03,\n",
      "          3.6436e-02,  1.4225e-02, -6.1062e-03, -1.6541e-02,  3.0664e-04,\n",
      "          5.0740e-02, -4.2370e-02, -2.3241e-02,  2.1088e-02,  8.0076e-02,\n",
      "          2.5034e-02,  1.8816e-02, -3.8452e-02, -5.8870e-02,  7.1720e-03,\n",
      "         -7.8927e-03, -1.0138e-02,  6.1291e-02,  5.0994e-02, -1.4389e-02,\n",
      "          7.6674e-02, -2.3729e-02,  4.7521e-02, -4.6062e-02, -3.0031e-02,\n",
      "          9.2691e-02, -2.4112e-02, -6.8115e-03,  1.2785e-02]])\n"
     ]
    }
   ],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['diabetes', 'type 2']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('tavakolih/all-MiniLM-L6-v2-pubmed-full')# pritamdeka/S-PubMedBert-MS-MARCO\n",
    "model = AutoModel.from_pretrained('tavakolih/all-MiniLM-L6-v2-pubmed-full')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e114d6d2-4b4b-400b-98c9-0322586b3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def create_quantized_index(embeddings_np, d):\n",
    "    \"\"\"Create a trained IVFPQ index.\"\"\"\n",
    "    nlist = 1000\n",
    "    m = 32\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "    index.train(embeddings_np)\n",
    "    return index\n",
    "\n",
    "\n",
    "#\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched_transformers(sentences, model_name=\"tavakolih/all-MiniLM-L6-v2-pubmed-full\"):\n",
    "    \"\"\"Return average embeddings for sentences using a Transformers model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling (mean pooling function is used here)\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa5c262-ec35-4a42-b8ce-08b590fddc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column_content(s):\n",
    "    \"\"\"Clean and strip unwanted characters and split by pipe if present.\"\"\"\n",
    "    # First, clean the string by removing specific patterns\n",
    "    cleaned = s.strip().lower()\n",
    "    \n",
    "    # Check if the cleaned string contains a pipe symbol and split if it does\n",
    "    if '|' in cleaned:\n",
    "        return cleaned.split('|')\n",
    "    else:\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT_FILENAME, usecols=['Class ID', 'Preferred Label', 'Synonyms', 'Definitions', 'alternative term'], \n",
    "                 sep=',', engine='python', on_bad_lines='skip')\n",
    "\n",
    "\n",
    "term_to_id = {}\n",
    "embeddings = []  \n",
    "indexed_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afca32f4-d356-427b-bdeb-790d6a96b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "for _, row in df.iterrows():\n",
    "    term_id = row['Class ID']\n",
    "    for col in ['Preferred Label', 'Synonyms', 'Definitions', 'alternative term']:\n",
    "        term_names = row[col]\n",
    "        if pd.notnull(term_names):  # Check if the term_name is not NaN\n",
    "            processed_terms = process_column_content(term_names)\n",
    "            if isinstance(processed_terms, list):\n",
    "                for term in processed_terms:\n",
    "                    flattened_data.append((term_id, term))\n",
    "            else:\n",
    "                flattened_data.append((term_id, processed_terms))\n",
    "\n",
    "# Convert flattened data to a DataFrame for easier manipulation\n",
    "flattened_df = pd.DataFrame(flattened_data, columns=['Class ID', 'Term Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f91e85c-bc7b-4074-aded-d8f1433863e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor d2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2r antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>runx1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33353</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>adrenal cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>tumor of the adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33355</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant neoplasm of adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33356</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant adrenal tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33357</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>an endocrine gland cancer located_in the adren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Class ID  \\\n",
       "0       http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "4      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "...                                            ...   \n",
       "33353     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33354     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33355     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33356     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33357     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "\n",
       "                                               Term Name  \n",
       "0                  adenosine phosphodiesterase inhibitor  \n",
       "1                        dopamine receptor d2 antagonist  \n",
       "2                                         d2r antagonist  \n",
       "3                                 d2 receptor antagonist  \n",
       "4                                        runx1 inhibitor  \n",
       "...                                                  ...  \n",
       "33353                                     adrenal cancer  \n",
       "33354                         tumor of the adrenal gland  \n",
       "33355                malignant neoplasm of adrenal gland  \n",
       "33356                            malignant adrenal tumor  \n",
       "33357  an endocrine gland cancer located_in the adren...  \n",
       "\n",
       "[33358 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56809e91-0ec4-453b-9d0d-74a0125bd329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terms: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33358/33358 [00:02<00:00, 15366.12it/s]\n",
      "Generating Embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 334/334 [10:00<00:00,  1.80s/it]\n",
      "WARNING clustering 33342 points to 1000 centroids: please provide at least 39000 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized faiss index...\n",
      "Saving term to ID mapping and indexed terms...\n",
      "Writing terms to a txt file...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "term_to_id = {}\n",
    "indexed_terms = []\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "term_batches = []\n",
    "id_batches = []\n",
    "current_batch_terms = []\n",
    "current_batch_ids = []\n",
    "\n",
    "for _, row in tqdm(flattened_df.iterrows(), total=flattened_df.shape[0], desc=\"Processing terms\"):\n",
    "    term_id = row['Class ID']\n",
    "    term_name = row['Term Name']\n",
    "\n",
    "    # Process the term_name\n",
    "    term_name = process_column_content(term_name)\n",
    "\n",
    "    # Check for empty or single character terms and skip them\n",
    "    if not term_name or len(term_name) <= 1:\n",
    "        continue\n",
    "\n",
    "    current_batch_terms.append(term_name)\n",
    "    current_batch_ids.append(term_id)\n",
    "\n",
    "    if len(current_batch_terms) == BATCH_SIZE:\n",
    "        term_batches.append(current_batch_terms)\n",
    "        id_batches.append(current_batch_ids)\n",
    "        current_batch_terms = []\n",
    "        current_batch_ids = []\n",
    "\n",
    "# Catch any remaining terms not added to a batch\n",
    "if current_batch_terms:\n",
    "    term_batches.append(current_batch_terms)\n",
    "    id_batches.append(current_batch_ids)\n",
    "\n",
    "for term_batch, id_batch in tqdm(zip(term_batches, id_batches), total=len(term_batches), desc=\"Generating Embeddings\"):\n",
    "    batch_embeddings = get_average_embeddings_batched_transformers(term_batch)\n",
    "    \n",
    "    for term, term_id, embedding in zip(term_batch, id_batch, batch_embeddings):\n",
    "        norm = np.linalg.norm(embedding)\n",
    "\n",
    "        # Check if the embedding is a zero vector\n",
    "        if norm == 0:\n",
    "            print(f\"Term '{term}' with ID '{term_id}' has a zero vector.\")\n",
    "\n",
    "        # Normalizing the vector\n",
    "        normalized_embedding = embedding if norm == 0 else embedding / norm\n",
    "        embeddings.append(normalized_embedding)\n",
    "        term_to_id[term] = term_id\n",
    "        indexed_terms.append(term)\n",
    "\n",
    "        # Clear out the current batch to free up memory\n",
    "    del term_batch, id_batch, batch_embeddings\n",
    "    gc.collect()\n",
    "\n",
    "# Assuming we have already calculated sentence_embeddings somewhere in the script\n",
    "d = embeddings[0].shape[0] if embeddings else 0  # Dynamically get the dimension\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "index = create_quantized_index(embeddings_np, d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Free up memory after using embeddings_np\n",
    "del embeddings, embeddings_np\n",
    "gc.collect()\n",
    "\n",
    "print(\"Saving quantized faiss index...\")\n",
    "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
    "\n",
    "# print(\"Saving term to ID mapping...\")\n",
    "# with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "#     pickle.dump(term_to_id, outfile)\n",
    "\n",
    "print(\"Saving term to ID mapping and indexed terms...\")\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "    pickle.dump({\"term_to_id\": term_to_id, \"indexed_terms\": indexed_terms}, outfile)\n",
    "\n",
    "\n",
    "print(\"Writing terms to a txt file...\")\n",
    "with open(OUTPUT_LIST, \"w\") as txt_file:\n",
    "    for term in term_to_id.keys():\n",
    "        txt_file.write(term + \"\\n\")\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c50507-c637-46a5-84f6-cac4d73f0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the term to ID mapping and indexed terms\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(FAISS_INDEX_FILENAME)\n",
    "\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    query = query.lower()  # Convert query to lowercase\n",
    "    query_embedding = get_average_embeddings_batched_transformers([query])[0].numpy()  # Get average embedding of the query\n",
    "\n",
    "    # Normalize the query embedding\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "\n",
    "    return similar_terms\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query_embedding = get_average_embeddings_batched_transformers([query])[0].numpy()  # Get average embedding of the query\n",
    "\n",
    "    # Normalize the query embedding\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "\n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "\n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "\n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49497dac-8c96-42d9-a4a1-5ecde5c17a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_suggestions(term):\n",
    "    query = term\n",
    "    results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "    for term, term_id, score in results:\n",
    "        print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2650ad28-6d99-439f-95e7-a7ecba4ae9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: surface plasmon resonance, ID: http://www.bioassayontology.org/bao#BAO_0000054, Score: 0.5017558336257935\n",
      "Term: one of several models of surface plasmon resonance (spr) instruments., ID: http://www.bioassayontology.org/bao#BAO_0150085, Score: 0.64713454246521\n",
      "Term: for surface plasmon resonance measurements, target protein is immobilized onto a solid surface via covalent or noncovalent interaction. binding of small molecule compounds changes the refractive index of the immobilized target, which can be detected by optical sensing. shift in wavelength of light is proportional to mass changes near the sensor surface, with sensitivity to detect mass change being ~300 da., ID: http://www.bioassayontology.org/bao#BAO_0000054, Score: 0.7980142831802368\n",
      "Term: resonant waveguide grating, ID: http://www.bioassayontology.org/bao#BAO_0000069, Score: 0.9245482683181763\n",
      "Term: excitation wavelength, ID: http://www.bioassayontology.org/bao#BAO_0000566, Score: 0.9980339407920837\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "show_suggestions('Surface plasmon resonating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36313926-0bdd-4ab3-a589-6314bf151868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: spr, ID: http://www.bioassayontology.org/bao#BAO_0000054, Score: 0.509190559387207\n",
      "Term: quantitative pcr, ID: http://www.bioassayontology.org/bao#BAO_0003031, Score: 0.8667311668395996\n",
      "Term: real-time pcr, ID: http://www.bioassayontology.org/bao#BAO_0002084, Score: 0.8763004541397095\n",
      "Term: taqman real-time pcr assays (thermo-fisher), ID: http://www.bioassayontology.org/bao#BAO_0140015, Score: 0.8813551068305969\n",
      "Term: physical tracer, ID: http://purl.obolibrary.org/obo/CHEBI_35208, Score: 0.893916130065918\n"
     ]
    }
   ],
   "source": [
    "show_suggestions('SPR analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c8063c-2777-4f9d-84c9-5992afdfc040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: tr-fret, ID: http://www.bioassayontology.org/bao#BAO_0000004, Score: 0.3012160360813141\n",
      "Term: cret, ID: http://www.bioassayontology.org/bao#BAO_0000462, Score: 0.7369927763938904\n",
      "Term: trupath, ID: http://www.bioassayontology.org/bao#BAO_0010081, Score: 0.7620601654052734\n",
      "Term: trna, ID: http://www.bioassayontology.org/bao#BAO_0000276, Score: 0.7682546973228455\n",
      "Term: thale-cress, ID: http://purl.obolibrary.org/obo/NCBITaxon_3702, Score: 0.7849703431129456\n"
     ]
    }
   ],
   "source": [
    "show_suggestions('TR-FRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cca0b39-c99f-4640-bd7b-1e63123e9c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: ion redistribution assay, ID: http://www.bioassayontology.org/bao#BAO_0000819, Score: 0.7139953374862671\n",
      "Term: metal ion redistribution assay, ID: http://www.bioassayontology.org/bao#BAO_0000162, Score: 0.7736241817474365\n",
      "Term: primary assay sar, ID: http://www.bioassayontology.org/bao#BAO_0000031, Score: 0.7854743003845215\n",
      "Term: signal transduction assay, ID: http://www.bioassayontology.org/bao#BAO_0003002, Score: 0.8055042624473572\n",
      "Term: this can be moved to directly under 'assay measurement'., ID: http://www.bioassayontology.org/bao#BAO_0002069, Score: 0.8106790781021118\n"
     ]
    }
   ],
   "source": [
    "show_suggestions('Radioligand displacement assay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442d259-ea53-4d8a-9099-27941aecfd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
