{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd7af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stirunag/.pyenv/versions/3.10.6/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3f6fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_floret_model = '/home/stirunag/work/github/source_data/floret_embeddings/en_floret_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7648e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-biomedner-europepmc 1.0.0 requires spacy<3.3.0,>=3.2.4, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install floret 'spacy~=3.4.0' pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9820c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# This is the spaCy pipeline with floret vectors\n",
    "nlp_fl = spacy.load(path_floret_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "152bc246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43325984477996826"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = nlp_fl.vocab[\"sars\"]\n",
    "word_2 = nlp_fl.vocab[\"sars-cove-2\"]\n",
    "\n",
    "word_1.similarity(word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2096d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sars cov 2 [-3.5737886  2.3306377  4.0841546] 55.919779433640024\n",
      "sars [ 2.3757248  -1.0899751   0.76489997] 42.306488\n",
      "cov [-1.0599601  1.1186376  3.3393645] 57.941483\n",
      "2 [-12.03713   6.96325   8.1482 ] 152.65103\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp_fl(\"sars cov 2\")\n",
    "    \n",
    "print(tokens.text, tokens.vector[:3], tokens.vector_norm) # Only the first three components of the vector \n",
    "    \n",
    "for token in tokens:\n",
    "    print(token.text, token.vector[:3], token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f1ded3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08433879911899567, 0.5982748866081238)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_embedding(term):\n",
    "    tokens = term.split()\n",
    "    # Get embeddings for each token\n",
    "    embeddings = [nlp_fl.vocab[token].vector for token in tokens if token in nlp_fl.vocab]\n",
    "    # Compute the average embedding\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "\n",
    "word_1 = nlp_fl.vocab[\"T2DM\"]\n",
    "word_2 = nlp_fl.vocab[\"neurofibromatosis\"]\n",
    "word_4 = nlp_fl.vocab[\"Type 2 Diabetes Mellitus T2DM\"]\n",
    "\n",
    "\n",
    "word_1.similarity(word_2), word_1.similarity(word_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcc93c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0843388\n",
      "0.5035879\n",
      "0.5035879\n"
     ]
    }
   ],
   "source": [
    "def get_average_embedding(term):\n",
    "    tokens = term.split()\n",
    "    embeddings = [nlp_fl.vocab[token].vector for token in tokens if token in nlp_fl.vocab]\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute cosine similarity between two vectors\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "word_1_vec = nlp_fl.vocab[\"T2DM\"].vector\n",
    "word_2_vec = nlp_fl.vocab[\"neurofibromatosis\"].vector\n",
    "word_3_vec = get_average_embedding(\"Type 2 Diabetes Mellitus T2DM\")\n",
    "# For word_4, we get the vector of the entire phrase\n",
    "word_4_doc = nlp_fl(\"Type 2 Diabetes Mellitus T2DM\")\n",
    "word_4_vec = word_4_doc.vector\n",
    "\n",
    "print(cosine_similarity(word_1_vec, word_2_vec))\n",
    "print(cosine_similarity(word_1_vec, word_3_vec))\n",
    "print(cosine_similarity(word_1_vec, word_4_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3db5a18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fac005ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"/home/stirunag/work/github/ML_annotations/normalisation/en_floret_model\")\n",
    "\n",
    "# def get_average_embedding(term):\n",
    "#     \"\"\"Get the average word embedding for a term.\"\"\"\n",
    "#     tokens = term.split()\n",
    "#     valid_vectors = [nlp.vocab[token].vector for token in tokens if\n",
    "#                      nlp.vocab[token].has_vector and nlp.vocab[token].vector.shape[0] == 300]\n",
    "\n",
    "#     if len(valid_vectors) == 0:\n",
    "#         return np.zeros((300,))\n",
    "\n",
    "#     average_embedding = np.mean(valid_vectors, axis=0)\n",
    "#     return average_embedding\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/chebi_terms.pkl\", \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/chebi_terms.index\")\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "    \n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "    \n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "    \n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d6af2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: fluensulfone, ID: CHEBI:131597, Score: 0.5945534110069275\n",
      "Term: amidosulfuron, ID: CHEBI:2635, Score: 0.6972651481628418\n",
      "Term: sulfadiasulfone, ID: CHEBI:135557, Score: 0.7089824080467224\n",
      "Term: metolachlor morpholinone, ID: CHEBI:83509, Score: 0.7168866395950317\n",
      "Term: dimethylvinphos, ID: CHEBI:38659, Score: 0.7179254293441772\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"gludosulfone\"\n",
    "results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "for term, term_id, score in results:\n",
    "    print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ba84b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: cholesterol, Score: 100\n",
      "Term: cholesterol_d7, Score: 88\n",
      "Term: cholestane, Score: 76\n",
      "Term: dihydroxycholesterol, Score: 71\n",
      "Term: sterol, Score: 71\n",
      "Term: oxysterol, Score: 70\n",
      "Term: sitosterol, Score: 67\n",
      "Term: lipoprotein cholesterol, Score: 65\n",
      "Term: cholesterol glucuronide, Score: 65\n",
      "Term: androstane sterol, Score: 57\n"
     ]
    }
   ],
   "source": [
    "# For demonstration:\n",
    "query = \"cholesterol\"\n",
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "37351934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"/home/stirunag/work/github/ML_annotations/normalisation/en_floret_model\")\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/uniprot_terms.pkl\", \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/uniprot_terms.index\")\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query = query\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "    \n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "    \n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "    \n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed9ff6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: cil-1, Score: 89\n",
      "Term: natSil-1, Score: 67\n",
      "Term: epl-1, Score: 67\n",
      "Term: blos-1, Score: 60\n",
      "Term: Msil_2912, Score: 46\n",
      "Term: Ccr1l1, Score: 40\n",
      "Term: Meg1, Score: 25\n",
      "Term: Atperox P61, Score: 13\n",
      "Term: bax, Score: 0\n",
      "Term: norpA, Score: 0\n"
     ]
    }
   ],
   "source": [
    "# For demonstration:\n",
    "query = \"IL-1\"\n",
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178cf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "de4cd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"/home/stirunag/work/github/ML_annotations/normalisation/en_floret_model\")\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/umls_terms.pkl\", \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/umls_terms.index\")\n",
    "\n",
    "indexed_terms_ids = [(term, term_to_id[term]) for term in indexed_terms]\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query with their IDs.\"\"\"\n",
    "    query = query\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the terms and their IDs from the indexed_terms_ids list\n",
    "    candidate_terms_and_ids = [indexed_terms_ids[i] for i in I[0]]\n",
    "    \n",
    "    # Split into separate lists for scoring\n",
    "    candidate_terms, candidate_ids = zip(*candidate_terms_and_ids)\n",
    "    \n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "    \n",
    "    # Pair up terms with their scores and IDs\n",
    "    term_score_id_triples = list(zip(candidate_terms, scores, candidate_ids))\n",
    "    \n",
    "    # Rank these triples based on scores\n",
    "    ranked_term_score_id_triples = sorted(term_score_id_triples, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_term_score_id_triples[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19967efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: Hemophagocytosis, Score: 100, ID: C0876991\n",
      "Term: Haemophagocytosis, Score: 97, ID: C0876991\n",
      "Term: Lymphocytosis, Score: 76, ID: C0853698\n",
      "Term: Hemophagocytic histiocytosis, Score: 73, ID: C5208355\n",
      "Term: Hemophagocytic lymphohistiocytosis, Score: 64, ID: C0024291\n",
      "Term: Hemophagocytic Lymphohistiocytosis, Score: 64, ID: C0024291\n",
      "Term: Hemophagocytic Lymphohistiocytoses, Score: 64, ID: C0024291\n",
      "Term: Haemophagocytic lymphohistiocytosis, Score: 63, ID: C0024291\n",
      "Term: Hyperleukocytosis, Score: 61, ID: C4324336\n",
      "Term: Granulocytosis, Score: 60, ID: C1282609\n"
     ]
    }
   ],
   "source": [
    "# For demonstration:\n",
    "query = \"Hemophagocytosis\"\n",
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score, term_id in results:\n",
    "    print(f\"Term: {term}, Score: {score}, ID: {term_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e7fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f72b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7f543c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.22.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.22.0\n",
      "  Downloading Levenshtein-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.9/172.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.22.0 fuzzywuzzy-0.18.0 python-Levenshtein-0.22.0 rapidfuzz-3.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40039898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"/home/stirunag/work/github/ML_annotations/normalisation/en_floret_model\")\n",
    "\n",
    "def get_average_embedding(term):\n",
    "    \"\"\"Get the average word embedding for a term.\"\"\"\n",
    "    tokens = term.split()\n",
    "    valid_vectors = [nlp.vocab[token].vector for token in tokens if\n",
    "                     nlp.vocab[token].has_vector and nlp.vocab[token].vector.shape[0] == 300]\n",
    "\n",
    "    if len(valid_vectors) == 0:\n",
    "        return np.zeros((300,))\n",
    "\n",
    "    average_embedding = np.mean(valid_vectors, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/NCBI_terms.pkl\", \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/NCBI_terms.index\")\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embedding(query)\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2717e58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: sars, ID: 694009, Score: 0.35455653071403503\n",
      "Term: sars coronavirus sin848, ID: 267387, Score: 0.5467997193336487\n",
      "Term: sars coronavirus sin842, ID: 267383, Score: 0.5532567501068115\n",
      "Term: sars coronavirus sin845, ID: 267395, Score: 0.5810052156448364\n",
      "Term: sars coronavirus sin846, ID: 267396, Score: 0.5851927995681763\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"sars\"\n",
    "results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "for term, term_id, score in results:\n",
    "    print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5834068d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_to_id['dsm 3960']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdd252eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Test the retrieval\u001b[39;00m\n\u001b[1;32m     47\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdsm 3960\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m similar_terms \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_similar_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m similar_terms:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Similarity Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 39\u001b[0m, in \u001b[0;36mretrieve_similar_terms\u001b[0;34m(query_term, k)\u001b[0m\n\u001b[1;32m     37\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 39\u001b[0m     term \u001b[38;5;241m=\u001b[39m \u001b[43mid_to_term\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Use the inverse mapping\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     cosine_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m distances[\u001b[38;5;241m0\u001b[39m][i])\n\u001b[1;32m     41\u001b[0m     term_id \u001b[38;5;241m=\u001b[39m term_to_id[term]\n",
      "\u001b[0;31mKeyError\u001b[0m: 36"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"/home/stirunag/work/github/ML_annotations/normalisation/en_floret_model\")\n",
    "\n",
    "def get_average_embedding(term):\n",
    "    doc = nlp(term.lower())\n",
    "    valid_vectors = [token.vector for token in doc if token.has_vector and token.vector.shape[0] == 300]\n",
    "    \n",
    "    if len(valid_vectors) == 0:\n",
    "        return np.zeros((300,))\n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "# Load term-to-ID mapping\n",
    "with open(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/NCBI_terms.pkl\", \"rb\") as infile:\n",
    "    term_to_id = pickle.load(infile)\n",
    "\n",
    "# Load the faiss index\n",
    "index = faiss.read_index(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/NCBI_terms.index\")\n",
    "\n",
    "def normalize_vector(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "# Create the inverse mapping\n",
    "id_to_term = {v: k for k, v in term_to_id.items()}\n",
    "\n",
    "def retrieve_similar_terms(query_term, k=5):\n",
    "    # Convert query term into vector\n",
    "    query_vector = normalize_vector(get_average_embedding(query_term)).reshape(1, -1).astype('float32')\n",
    "    \n",
    "    # Search the index for top k similar terms\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        term = id_to_term[idx]  # Use the inverse mapping\n",
    "        cosine_similarity = max(0, 1 - 0.5 * distances[0][i])\n",
    "        term_id = term_to_id[term]\n",
    "        results.append({'term': term, 'id': term_id, 'similarity': cosine_similarity})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the retrieval\n",
    "query = \"dsm 3960\"\n",
    "similar_terms = retrieve_similar_terms(query)\n",
    "for entry in similar_terms:\n",
    "    print(f\"Term: {entry['term']}, ID: {entry['id']}, Similarity Score: {entry['similarity']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b8a8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# term = \"infectious bursal disease virus cu-1\"\n",
    "# vec = get_average_embedding(term)\n",
    "# print(vec)\n",
    "# print(np.linalg.norm(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29c6ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embeddings_batched(terms):\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        valid_vectors = [token.vector for token in doc if \n",
    "                         token.has_vector and token.vector.shape[0] == 300]\n",
    "        \n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Example of usage:\n",
    "terms = [\"Bacteria virus cu-1\", \"Another example term\"]  # List of terms you want to process\n",
    "embeddings = get_average_embeddings_batched(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1079d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9233815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM:\n",
      "[[ 1.  2.  3.]\n",
      " [ 2.  3.  4.]\n",
      " [ 3.  4.  5.]\n",
      " [ 4.  5.  6.]\n",
      " [ 5.  6.  7.]\n",
      " [ 6.  7.  8.]\n",
      " [ 7.  8.  9.]\n",
      " [ 8.  9. 10.]]\n",
      "K: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Embed(A, L):\n",
    "    T = len(A)\n",
    "    K = T - L + 1\n",
    "    N = len(A)\n",
    "    X = np.zeros((L, K))\n",
    "    \n",
    "    for i in range(K):\n",
    "        X[:, i] = A[i:L+i]\n",
    "    \n",
    "    TM = np.hstack([X[:, 0].reshape(-1, 1), X])\n",
    "    TM = TM.T\n",
    "    \n",
    "    return TM, K\n",
    "\n",
    "# Example of use:\n",
    "A = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "L = 3\n",
    "TM, K = Embed(A, L)\n",
    "print(\"TM:\")\n",
    "print(TM[1::])\n",
    "print(\"K:\", K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b0f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "907d6a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def Hankelize(TM):\n",
    "    N, M = TM.shape\n",
    "\n",
    "    # Prepare an array to store summation and count of each diagonal\n",
    "    summation = np.zeros(N + M - 1)\n",
    "    count = np.zeros(N + M - 1)\n",
    "\n",
    "    # Iterate over the TM matrix to populate the summation and count arrays\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            summation[i + j] += TM[i, j]\n",
    "            count[i + j] += 1\n",
    "\n",
    "    # Element-wise division to get the average\n",
    "    HM = summation / count\n",
    "\n",
    "    return HM\n",
    "\n",
    "\n",
    "# Example of use:\n",
    "\n",
    "result = Hankelize(TM[1::])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dde990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
