{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e3711a-a8c9-480d-8149-005bc34b1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bad8182-4f14-4ba9-be65-1e3a975e645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "\n",
    "path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/source_data/dictionaries/\"\n",
    "# path__ = \"/homes/ines\"\n",
    "INPUT_FILENAME = \"/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/source_data/knowledge_base/bao/BAO.csv\"\n",
    "OUTPUT_PICKLE_FILENAME = path__ + \"bao.pkl\"\n",
    "OUTPUT_LIST = path__ + \"bao_list.txt\"\n",
    "FAISS_INDEX_FILENAME = path__ + \"bao_terms.index\"\n",
    "# OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9b63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bioformers/bioformer-8L')\n",
    "model = BertModel.from_pretrained('bioformers/bioformer-8L')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize input text and convert to tensors\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    # Get hidden states from BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the embeddings from the last hidden layer\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "    # Perform mean pooling by averaging token embeddings across all tokens\n",
    "    mean_embedding = hidden_states.mean(dim=1).squeeze()\n",
    "    \n",
    "    return mean_embedding.numpy()\n",
    "\n",
    "# def get_average_embeddings_batched_transformers(sentences, model_name=\"tavakolih/all-MiniLM-L6-v2-pubmed-full\"):\n",
    "#     \"\"\"Return average embeddings for sentences using a Transformers model.\"\"\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#     # Tokenize sentences\n",
    "#     encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#     # Compute token embeddings\n",
    "#     with torch.no_grad():\n",
    "#         model_output = model(**encoded_input)\n",
    "\n",
    "#     # Perform pooling (mean pooling function is used here)\n",
    "#     def mean_pooling(model_output, attention_mask):\n",
    "#         token_embeddings = model_output[0]  # First element of model_output contains token embeddings\n",
    "#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#         sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "#         sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "#         return sum_embeddings / sum_mask\n",
    "\n",
    "#     sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "#     # Normalize embeddings\n",
    "#     sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "#     return sentence_embeddings\n",
    "\n",
    "def create_index(embeddings_np, d):\n",
    "    \"\"\"Create an accurate Faiss index using L2 distance.\"\"\"\n",
    "    index = faiss.IndexFlatL2(d)  # No quantization, exact search\n",
    "    index.add(embeddings_np)  # Directly add embeddings to the index\n",
    "    return index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ffd2abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor d2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2r antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>runx1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33353</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>adrenal cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>tumor of the adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33355</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant neoplasm of adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33356</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant adrenal tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33357</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>an endocrine gland cancer located_in the adren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Class ID  \\\n",
       "0       http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "4      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "...                                            ...   \n",
       "33353     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33354     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33355     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33356     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33357     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "\n",
       "                                               Term Name  \n",
       "0                  adenosine phosphodiesterase inhibitor  \n",
       "1                        dopamine receptor d2 antagonist  \n",
       "2                                         d2r antagonist  \n",
       "3                                 d2 receptor antagonist  \n",
       "4                                        runx1 inhibitor  \n",
       "...                                                  ...  \n",
       "33353                                     adrenal cancer  \n",
       "33354                         tumor of the adrenal gland  \n",
       "33355                malignant neoplasm of adrenal gland  \n",
       "33356                            malignant adrenal tumor  \n",
       "33357  an endocrine gland cancer located_in the adren...  \n",
       "\n",
       "[33358 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_column_content(s):\n",
    "    \"\"\"Clean and strip unwanted characters and split by pipe if present.\"\"\"\n",
    "    # First, clean the string by removing specific patterns\n",
    "    cleaned = s.strip().lower()\n",
    "    \n",
    "    # Check if the cleaned string contains a pipe symbol and split if it does\n",
    "    if '|' in cleaned:\n",
    "        return cleaned.split('|')\n",
    "    else:\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT_FILENAME, usecols=['Class ID', 'Preferred Label', 'Synonyms', 'Definitions', 'alternative term'], \n",
    "                 sep=',', engine='python', on_bad_lines='skip')\n",
    "\n",
    "\n",
    "term_to_id = {}\n",
    "embeddings = []  \n",
    "indexed_terms = []\n",
    "\n",
    "\n",
    "flattened_data = []\n",
    "for _, row in df.iterrows():\n",
    "    term_id = row['Class ID']\n",
    "    for col in ['Preferred Label', 'Synonyms', 'Definitions', 'alternative term']:\n",
    "        term_names = row[col]\n",
    "        if pd.notnull(term_names):  # Check if the term_name is not NaN\n",
    "            processed_terms = process_column_content(term_names)\n",
    "            if isinstance(processed_terms, list):\n",
    "                for term in processed_terms:\n",
    "                    flattened_data.append((term_id, term))\n",
    "            else:\n",
    "                flattened_data.append((term_id, processed_terms))\n",
    "\n",
    "# Convert flattened data to a DataFrame for easier manipulation\n",
    "flattened_df = pd.DataFrame(flattened_data, columns=['Class ID', 'Term Name'])\n",
    "\n",
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386bf802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terms:   0%|                             | 0/33358 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing terms: 100%|████████████████| 33358/33358 [03:43<00:00, 149.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV into a DataFrame\n",
    "df = flattened_df\n",
    "\n",
    "# Preprocess and collect embeddings\n",
    "bert_embeddings = []\n",
    "term_to_id = {}\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing terms\"):\n",
    "    term_id = row['Class ID']\n",
    "    term_name = row['Term Name']\n",
    "    \n",
    "    term_to_id[term_name] = term_id\n",
    "    embedding = get_bert_embedding(term_name)\n",
    "    bert_embeddings.append(embedding)\n",
    "\n",
    "# Convert list of embeddings to a NumPy array\n",
    "embeddings_np = np.array(bert_embeddings)\n",
    "d = embeddings_np.shape[1]  # Dimensionality of embeddings\n",
    "\n",
    "# Create and save the Faiss index\n",
    "index = create_index(embeddings_np, d)\n",
    "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
    "\n",
    "# Save term to ID mapping\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "    pickle.dump({\"term_to_id\": term_to_id}, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86bee9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write terms to a text file\n",
    "with open(OUTPUT_LIST, \"w\") as txt_file:\n",
    "    for term in term_to_id.keys():\n",
    "        txt_file.write(term + \"\\n\")\n",
    "\n",
    "def match_entity(ner_entity):\n",
    "    # Step 1: TF-IDF matching using 2-grams and 3-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['Term Name'])\n",
    "    query_vector = tfidf_vectorizer.transform([ner_entity])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    best_match_idx = similarities.argmax()\n",
    "    best_match_score = similarities.max()\n",
    "    \n",
    "    if best_match_score > 0.0:\n",
    "        matched_term_name = df.iloc[best_match_idx]['Term Name']\n",
    "        matched_term_id = df.iloc[best_match_idx]['Class ID']\n",
    "        return matched_term_id, matched_term_name, best_match_score\n",
    "    \n",
    "    # Step 2: BERT + FAISS if no good TF-IDF match\n",
    "    query_embedding = get_bert_embedding(ner_entity)\n",
    "    distances, indices = index.search(np.array([query_embedding]), k=1)\n",
    "    \n",
    "    matched_term_name = df.iloc[indices[0][0]]['Term Name']\n",
    "    matched_term_id = df.iloc[indices[0][0]]['Class ID']\n",
    "    score = 1 - distances[0][0]\n",
    "    \n",
    "    return matched_term_id, matched_term_name, score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "130bd280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Entity: 'SPR Analysis' -> Matched Term Name: 'bradford protein assay', Term ID: http://www.bioassayontology.org/bao#BAO_0002463, Score: -48.478981018066406\n",
      "NER Entity: 'TR-FRET method' -> Matched Term Name: 'tr-fret', Term ID: http://www.bioassayontology.org/bao#BAO_0000004, Score: 1.0\n",
      "NER Entity: 'Radioligand displacement' -> Matched Term Name: 'potentials of mean force scoring function', Term ID: http://www.bioassayontology.org/bao#BAO_0002404, Score: -91.42115783691406\n"
     ]
    }
   ],
   "source": [
    "# List of NER entities\n",
    "ner_entities = [\"SPR Analysis\", \"TR-FRET method\", \"Radioligand displacement\"]\n",
    "\n",
    "# Loop over each NER entity and find the match\n",
    "for ner_entity in ner_entities:\n",
    "    term_id, term_name, score = match_entity(ner_entity)\n",
    "    print(f\"NER Entity: '{ner_entity}' -> Matched Term Name: '{term_name}', Term ID: {term_id}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory after use\n",
    "del embeddings_np\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855336a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
