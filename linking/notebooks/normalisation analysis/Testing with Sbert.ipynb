{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f294340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 0.0352, -0.0610,  0.0081,  ..., -0.0234, -0.0204, -0.0256],\n",
      "        [ 0.0725, -0.0013,  0.0417,  ...,  0.0076,  0.0047, -0.0276]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME='FremyCompany/BioLORD-2023'\n",
    "\n",
    "import torch\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['T2D', 'diabetes']\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # ('tavakolih/all-MiniLM-L6-v2-pubmed-full')\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d874af59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aba3c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Euclidean Distance: 1.147909164428711\n",
      "Cosine Similarity: 0.3411523103713989\n"
     ]
    }
   ],
   "source": [
    "euclidean_distance = F.pairwise_distance(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0))\n",
    "\n",
    "# Compute cosine similarity between the two sentence embeddings\n",
    "cosine_similarity = F.cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0))\n",
    "\n",
    "print(\"\\nEuclidean Distance:\", euclidean_distance.item())\n",
    "print(\"Cosine Similarity:\", cosine_similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92552bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pronto import Ontology\n",
    "import spacy\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import re\n",
    "import pandas\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def create_quantized_index(embeddings_np, d, nlist):\n",
    "    \"\"\"Create a trained IVFPQ index.\"\"\"\n",
    "    m = 32\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "    index.train(embeddings_np)\n",
    "    return index\n",
    "\n",
    "\n",
    "#\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched_transformers(sentences, model_name=MODEL_NAME):\n",
    "    \"\"\"Return average embeddings for sentences using a Transformers model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling (mean pooling function is used here)\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96ae3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3615a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "\n",
    "path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/\"\n",
    "INPUT_FILENAME = path__+\"work/github/source_data/knowledge_base/bao/BAO.csv\"\n",
    "OUTPUT_PICKLE_FILENAME = path__+\"/work/github/CAPITAL/normalisation/dictionary/bao.pkl\"\n",
    "OUTPUT_LIST = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_list.txt\"\n",
    "FAISS_INDEX_FILENAME = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_terms.index\"\n",
    "# OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef8e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column_content(s):\n",
    "    \"\"\"Clean and strip unwanted characters and split by pipe if present.\"\"\"\n",
    "    # First, clean the string by removing specific patterns\n",
    "    cleaned = s.strip().lower()\n",
    "    \n",
    "    # Check if the cleaned string contains a pipe symbol and split if it does\n",
    "    if '|' in cleaned:\n",
    "        return cleaned.split('|')\n",
    "    else:\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT_FILENAME, usecols=['Class ID', 'Preferred Label', 'Synonyms', 'Definitions', 'alternative term'], \n",
    "                 sep=',', engine='python', on_bad_lines='skip')\n",
    "\n",
    "\n",
    "term_to_id = {}\n",
    "embeddings = []  \n",
    "indexed_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b69b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "for _, row in df.iterrows():\n",
    "    term_id = row['Class ID']\n",
    "    for col in ['Preferred Label', 'Synonyms', 'Definitions', 'alternative term']:\n",
    "        term_names = row[col]\n",
    "        if pd.notnull(term_names):  # Check if the term_name is not NaN\n",
    "            processed_terms = process_column_content(term_names)\n",
    "            if isinstance(processed_terms, list):\n",
    "                for term in processed_terms:\n",
    "                    flattened_data.append((term_id, term))\n",
    "            else:\n",
    "                flattened_data.append((term_id, processed_terms))\n",
    "\n",
    "# Convert flattened data to a DataFrame for easier manipulation\n",
    "flattened_df = pd.DataFrame(flattened_data, columns=['Class ID', 'Term Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0edca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor d2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2r antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>runx1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33353</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>adrenal cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>tumor of the adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33355</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant neoplasm of adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33356</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant adrenal tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33357</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>an endocrine gland cancer located_in the adren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Class ID  \\\n",
       "0       http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "4      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "...                                            ...   \n",
       "33353     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33354     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33355     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33356     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33357     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "\n",
       "                                               Term Name  \n",
       "0                  adenosine phosphodiesterase inhibitor  \n",
       "1                        dopamine receptor d2 antagonist  \n",
       "2                                         d2r antagonist  \n",
       "3                                 d2 receptor antagonist  \n",
       "4                                        runx1 inhibitor  \n",
       "...                                                  ...  \n",
       "33353                                     adrenal cancer  \n",
       "33354                         tumor of the adrenal gland  \n",
       "33355                malignant neoplasm of adrenal gland  \n",
       "33356                            malignant adrenal tumor  \n",
       "33357  an endocrine gland cancer located_in the adren...  \n",
       "\n",
       "[33358 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5213962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor d2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2r antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>runx1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_194423</td>\n",
       "      <td>aquaretics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_194423</td>\n",
       "      <td>aquaretic agents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_194423</td>\n",
       "      <td>aquaretic agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_77703</td>\n",
       "      <td>ec 4.3.1.3 (histidine ammonia-lyase) inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_77703</td>\n",
       "      <td>histidine ammonia-lyase (ec 4.3.1.3) inhibitors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Class ID  \\\n",
       "0     http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1    http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2    http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3    http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "4    http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "..                                           ...   \n",
       "995  http://purl.obolibrary.org/obo/CHEBI_194423   \n",
       "996  http://purl.obolibrary.org/obo/CHEBI_194423   \n",
       "997  http://purl.obolibrary.org/obo/CHEBI_194423   \n",
       "998   http://purl.obolibrary.org/obo/CHEBI_77703   \n",
       "999   http://purl.obolibrary.org/obo/CHEBI_77703   \n",
       "\n",
       "                                           Term Name  \n",
       "0              adenosine phosphodiesterase inhibitor  \n",
       "1                    dopamine receptor d2 antagonist  \n",
       "2                                     d2r antagonist  \n",
       "3                             d2 receptor antagonist  \n",
       "4                                    runx1 inhibitor  \n",
       "..                                               ...  \n",
       "995                                       aquaretics  \n",
       "996                                 aquaretic agents  \n",
       "997                                  aquaretic agent  \n",
       "998   ec 4.3.1.3 (histidine ammonia-lyase) inhibitor  \n",
       "999  histidine ammonia-lyase (ec 4.3.1.3) inhibitors  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df = flattened_df[0:1000]\n",
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terms: 100%|██████████████████| 1000/1000 [00:00<00:00, 18422.57it/s]\n",
      "Generating Embeddings: 100%|████████████████████| 10/10 [01:32<00:00,  9.28s/it]\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 999 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "term_to_id = {}\n",
    "indexed_terms = []\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NLIST = 10\n",
    "term_batches = []\n",
    "id_batches = []\n",
    "current_batch_terms = []\n",
    "current_batch_ids = []\n",
    "\n",
    "for _, row in tqdm(flattened_df.iterrows(), total=flattened_df.shape[0], desc=\"Processing terms\"):\n",
    "    term_id = row['Class ID']\n",
    "    term_name = row['Term Name']\n",
    "\n",
    "    # Process the term_name\n",
    "    term_name = process_column_content(term_name)\n",
    "\n",
    "    # Check for empty or single character terms and skip them\n",
    "    if not term_name or len(term_name) <= 1:\n",
    "        continue\n",
    "\n",
    "    current_batch_terms.append(term_name)\n",
    "    current_batch_ids.append(term_id)\n",
    "\n",
    "    if len(current_batch_terms) == BATCH_SIZE:\n",
    "        term_batches.append(current_batch_terms)\n",
    "        id_batches.append(current_batch_ids)\n",
    "        current_batch_terms = []\n",
    "        current_batch_ids = []\n",
    "\n",
    "# Catch any remaining terms not added to a batch\n",
    "if current_batch_terms:\n",
    "    term_batches.append(current_batch_terms)\n",
    "    id_batches.append(current_batch_ids)\n",
    "\n",
    "for term_batch, id_batch in tqdm(zip(term_batches, id_batches), total=len(term_batches), desc=\"Generating Embeddings\"):\n",
    "    batch_embeddings = get_average_embeddings_batched_transformers(term_batch)\n",
    "    \n",
    "    for term, term_id, embedding in zip(term_batch, id_batch, batch_embeddings):\n",
    "        norm = np.linalg.norm(embedding)\n",
    "\n",
    "        # Check if the embedding is a zero vector\n",
    "        if norm == 0:\n",
    "            print(f\"Term '{term}' with ID '{term_id}' has a zero vector.\")\n",
    "\n",
    "        # Normalizing the vector\n",
    "        normalized_embedding = embedding if norm == 0 else embedding / norm\n",
    "        embeddings.append(normalized_embedding)\n",
    "        term_to_id[term] = term_id\n",
    "        indexed_terms.append(term)\n",
    "\n",
    "        # Clear out the current batch to free up memory\n",
    "    del term_batch, id_batch, batch_embeddings\n",
    "    gc.collect()\n",
    "\n",
    "# Assuming we have already calculated sentence_embeddings somewhere in the script\n",
    "d = embeddings[0].shape[0] if embeddings else 0  # Dynamically get the dimension\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "index = create_quantized_index(embeddings_np, d, nlist=NLIST)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Free up memory after using embeddings_np\n",
    "del embeddings, embeddings_np\n",
    "gc.collect()\n",
    "\n",
    "print(\"Saving quantized faiss index...\")\n",
    "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
    "\n",
    "# print(\"Saving term to ID mapping...\")\n",
    "# with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "#     pickle.dump(term_to_id, outfile)\n",
    "\n",
    "print(\"Saving term to ID mapping and indexed terms...\")\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "    pickle.dump({\"term_to_id\": term_to_id, \"indexed_terms\": indexed_terms}, outfile)\n",
    "\n",
    "\n",
    "print(\"Writing terms to a txt file...\")\n",
    "with open(OUTPUT_LIST, \"w\") as txt_file:\n",
    "    for term in term_to_id.keys():\n",
    "        txt_file.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cb867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb53f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a5e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a333fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filenames\n",
    "\n",
    "# path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/\"\n",
    "# OUTPUT_PICKLE_FILENAME = path__+\"/work/github/source_data/dictionary/bao.pkl\"\n",
    "# OUTPUT_LIST = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_list.txt\"\n",
    "# FAISS_INDEX_FILENAME = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_terms.index\"\n",
    "# # OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f659d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/production/literature/santosh_tirunagari/BACKUP//work/github/source_data/dictionaries/bao.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the term to ID mapping and indexed terms\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_PICKLE_FILENAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(infile)\n\u001b[1;32m     10\u001b[0m     term_to_id \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm_to_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/production/literature/santosh_tirunagari/BACKUP//work/github/source_data/dictionaries/bao.pkl'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(FAISS_INDEX_FILENAME)\n",
    "\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    query = query.lower()  # Convert query to lowercase\n",
    "    query_embedding = get_average_embeddings_batched_transformers([query])[0].numpy()  # Get average embedding of the query\n",
    "\n",
    "    # Normalize the query embedding\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "\n",
    "    return similar_terms\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query_embedding = get_average_embeddings_batched_transformers([query])[0].numpy()  # Get average embedding of the query\n",
    "\n",
    "    # Normalize the query embedding\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "\n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "\n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "\n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52aff044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: tr-fret, ID: http://www.bioassayontology.org/bao#BAO_0000004, Score: 0.3012159466743469\n",
      "Term: cret, ID: http://www.bioassayontology.org/bao#BAO_0000462, Score: 0.7369926571846008\n",
      "Term: trupath, ID: http://www.bioassayontology.org/bao#BAO_0010081, Score: 0.7620600461959839\n",
      "Term: trna, ID: http://www.bioassayontology.org/bao#BAO_0000276, Score: 0.7682545781135559\n",
      "Term: thale-cress, ID: http://purl.obolibrary.org/obo/NCBITaxon_3702, Score: 0.784970223903656\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"TR-FRET\"# \"nucleosome\"\n",
    "results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "for term, term_id, score in results:\n",
    "    print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce428e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: tr-fret, Score: 14\n",
      "Term: cin-quin, Score: 13\n",
      "Term: thale-cress, Score: 11\n",
      "Term: cret, Score: 0\n",
      "Term: trupath, Score: 0\n",
      "Term: trna, Score: 0\n",
      "Term: tcep, Score: 0\n",
      "Term: thale cress, Score: 0\n",
      "Term: gullet, Score: 0\n",
      "Term: acumen, Score: 0\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61f33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
