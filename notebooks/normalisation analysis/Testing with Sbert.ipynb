{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f294340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 7.0737e-02,  7.8338e-02,  4.9990e-02,  7.9972e-02,  2.9401e-02,\n",
      "         -4.0096e-03,  4.5121e-02, -7.6672e-03,  7.1605e-02,  2.6311e-02,\n",
      "          8.2655e-02, -4.1875e-02,  3.7981e-02, -1.8570e-02,  2.8644e-02,\n",
      "         -1.1675e-02,  4.5046e-02, -6.6515e-02, -1.3737e-01,  3.2388e-02,\n",
      "          2.4061e-02,  2.4938e-02,  1.5439e-02,  7.6069e-03, -4.2304e-02,\n",
      "         -1.2761e-02, -3.5186e-02,  5.2777e-02,  1.2653e-01, -3.4279e-02,\n",
      "         -6.3713e-02, -7.5150e-05,  3.4085e-02,  4.3905e-02, -4.2648e-03,\n",
      "          4.3988e-02, -1.1019e-02,  5.8703e-02, -1.3213e-02,  3.7645e-03,\n",
      "         -7.3039e-03, -4.8240e-02, -1.1894e-02, -1.6475e-02,  3.0159e-02,\n",
      "         -6.6944e-02,  4.8238e-03,  1.6012e-02,  2.0572e-02, -3.0864e-02,\n",
      "         -1.1507e-01, -5.0351e-02, -9.6909e-02, -1.5627e-02,  4.8998e-03,\n",
      "          2.8041e-02, -2.3860e-02,  7.4602e-02,  2.3521e-02, -2.4014e-02,\n",
      "          2.5796e-02, -6.9635e-03, -9.5127e-02,  2.7558e-02,  1.6430e-01,\n",
      "         -3.3154e-02, -2.7316e-03, -4.4246e-02, -8.5432e-02,  9.9791e-02,\n",
      "          6.1866e-02,  3.4661e-02, -3.8142e-02,  3.8041e-03, -7.2987e-02,\n",
      "         -7.2578e-02,  3.5389e-02, -4.3772e-02,  1.0639e-01,  4.9533e-02,\n",
      "         -8.9639e-02, -6.9472e-02, -1.7610e-02,  2.6072e-02,  4.6904e-02,\n",
      "         -2.3631e-02,  1.8314e-02, -1.4830e-01, -1.9894e-02,  5.3444e-03,\n",
      "         -1.4170e-02, -5.1089e-02,  2.2732e-02, -2.0768e-02,  2.2965e-02,\n",
      "         -2.8152e-02, -4.3727e-02, -3.9408e-02, -1.8677e-02,  1.1948e-01,\n",
      "          1.8846e-03,  4.8311e-02,  5.1914e-02,  4.6628e-02, -1.9640e-02,\n",
      "         -1.2108e-01, -3.5687e-02, -6.3689e-02,  1.2833e-02, -1.9320e-02,\n",
      "         -7.8841e-02, -4.7409e-02, -2.2215e-02,  5.0101e-03, -2.8634e-02,\n",
      "         -8.1907e-02, -1.0639e-01, -6.4804e-03, -8.3473e-02,  3.8894e-02,\n",
      "          3.9284e-02,  5.2713e-02, -7.7622e-02,  9.8934e-03, -5.6823e-02,\n",
      "         -5.9952e-02,  2.8178e-02, -5.2535e-07,  2.3328e-02, -6.9672e-02,\n",
      "         -5.2354e-02,  2.2603e-02,  5.5209e-02, -5.0704e-02, -5.2382e-02,\n",
      "          3.1742e-03, -7.4256e-03, -2.8632e-02,  5.9353e-03, -4.2853e-03,\n",
      "          2.7954e-02,  3.5012e-02,  6.8053e-02,  5.1727e-02, -6.3903e-02,\n",
      "          1.1888e-01, -2.2734e-02,  4.4499e-02, -2.1694e-02,  4.1798e-02,\n",
      "         -9.3615e-03, -2.7800e-02, -4.4863e-02, -3.4724e-02,  1.9883e-02,\n",
      "          8.2738e-03, -3.2600e-02,  5.2133e-02,  6.3435e-02,  3.9180e-02,\n",
      "         -5.9909e-02, -3.3455e-03,  5.4509e-02,  6.2581e-02,  7.7821e-02,\n",
      "         -5.0639e-02,  5.4242e-03,  2.0566e-02, -9.9871e-03, -5.2936e-02,\n",
      "          3.0802e-02, -3.0222e-02,  9.6884e-02,  4.5815e-02,  2.3005e-03,\n",
      "          2.3822e-02, -7.6491e-02,  1.4301e-02, -1.2375e-02,  5.7099e-02,\n",
      "          2.4674e-02, -2.1774e-02,  8.6764e-02,  3.2361e-02,  4.2554e-02,\n",
      "          3.1389e-02, -2.1952e-02, -2.0443e-02, -1.1298e-02,  4.4403e-02,\n",
      "         -6.6988e-02,  6.3899e-02, -1.4891e-02,  3.5125e-02,  2.4182e-02,\n",
      "         -7.4129e-02,  4.1386e-02,  1.4536e-02, -3.6202e-02,  3.8791e-03,\n",
      "         -6.8172e-02,  8.2606e-02, -4.7221e-02, -4.2665e-02, -2.5991e-02,\n",
      "         -1.6591e-02, -2.5811e-02,  1.2142e-02, -2.9231e-02, -1.3502e-01,\n",
      "          3.1402e-02,  3.3987e-02, -5.6515e-02,  1.5581e-02,  3.4290e-02,\n",
      "         -9.0457e-02, -3.3341e-02,  3.4099e-02, -3.7117e-02, -1.8251e-02,\n",
      "          5.1458e-02, -3.5054e-03,  2.0299e-02,  2.3339e-06, -7.1910e-02,\n",
      "          3.7132e-02, -9.1491e-02,  4.0670e-02,  9.8458e-02, -2.4476e-04,\n",
      "          5.1870e-02, -9.1918e-02,  9.2534e-03,  7.2785e-02, -9.6385e-02,\n",
      "          1.4935e-02,  4.5442e-02,  2.0681e-03,  3.6569e-02,  2.0527e-03,\n",
      "          1.2343e-01,  9.8454e-03, -5.1232e-03,  3.3492e-02, -3.2677e-02,\n",
      "          6.9393e-02, -1.6812e-02,  6.4395e-02, -6.7811e-02,  3.6029e-02,\n",
      "         -4.7370e-02, -3.6209e-02, -1.1052e-01, -3.5135e-02,  1.4038e-02,\n",
      "         -8.3996e-02, -4.4835e-02, -2.0524e-02, -2.4855e-02,  5.1377e-02,\n",
      "          2.4764e-02, -3.0651e-02, -5.9892e-02,  1.0020e-02, -1.0096e-03,\n",
      "         -2.1998e-02,  2.2467e-02,  8.8779e-02,  4.2233e-02, -5.9789e-02,\n",
      "         -2.9043e-02, -7.3402e-02, -1.0840e-02,  4.9322e-02, -9.8259e-02,\n",
      "          3.8624e-02, -5.1117e-02, -1.2021e-02, -9.1128e-02,  1.2401e-02,\n",
      "         -5.3569e-03, -7.6257e-02,  1.8452e-02,  4.2183e-02, -5.5513e-02,\n",
      "          3.0347e-02, -3.5938e-02,  7.1075e-02,  9.8298e-02, -3.7280e-02,\n",
      "         -5.8076e-02,  4.5175e-02,  2.3967e-02,  3.5976e-02,  1.1383e-01,\n",
      "          1.3983e-02, -1.5964e-01, -5.2647e-02, -7.1601e-02, -4.3872e-02,\n",
      "         -6.9379e-02, -8.2373e-03, -6.8981e-02, -5.9207e-02, -1.6039e-03,\n",
      "         -3.3739e-02,  4.6561e-02, -2.1774e-02, -1.5402e-02, -6.4263e-02,\n",
      "         -2.3919e-02, -6.1416e-02, -9.3034e-03, -2.1774e-02,  1.4569e-02,\n",
      "         -4.8410e-02,  6.2433e-02,  4.7340e-02, -2.7206e-03,  3.0723e-06,\n",
      "         -1.4930e-02, -3.2717e-02,  5.1297e-02, -4.4785e-02,  5.3463e-02,\n",
      "         -1.3895e-02,  8.1343e-02, -1.2077e-03, -1.2043e-02, -2.4182e-02,\n",
      "          2.3241e-02,  6.3372e-02, -3.0813e-02,  3.8136e-02,  4.9029e-02,\n",
      "          3.6203e-02,  1.6842e-02, -1.7357e-02, -3.0961e-02,  7.3934e-02,\n",
      "         -4.2715e-02,  2.1026e-02, -3.2637e-03,  3.6781e-03,  3.5932e-02,\n",
      "         -2.7353e-03,  2.8250e-02,  8.3349e-02,  5.9913e-03,  3.4154e-02,\n",
      "          5.2189e-02,  1.4623e-01,  1.8863e-02,  2.6524e-02,  3.2232e-02,\n",
      "          3.7782e-02,  5.0166e-02, -3.2829e-02,  6.1680e-02, -6.5041e-02,\n",
      "         -5.5734e-03,  6.8111e-03, -4.2371e-02,  1.2002e-01,  2.0498e-02,\n",
      "          3.1517e-02, -1.4240e-02, -1.4209e-02, -4.0205e-02,  1.9673e-02,\n",
      "         -8.7143e-04,  5.2427e-02,  4.0289e-02, -1.5397e-03,  4.2747e-02,\n",
      "          2.9128e-02,  3.7253e-02, -2.1438e-02, -4.4320e-02, -3.8695e-03,\n",
      "          2.1196e-03,  3.7441e-02,  6.2164e-02, -3.9469e-02],\n",
      "        [ 8.4419e-02,  1.1536e-01,  2.2928e-02,  6.1253e-03, -7.1574e-03,\n",
      "          3.4115e-02,  5.5405e-02, -2.3251e-02,  5.7799e-02,  1.5658e-02,\n",
      "          9.4563e-02, -8.7102e-02,  6.1687e-03, -5.3030e-02,  1.1286e-02,\n",
      "          2.6190e-02, -7.6942e-02,  5.6028e-02, -1.1688e-01, -5.8176e-02,\n",
      "          7.3042e-02,  8.6064e-02, -6.9512e-03,  9.1604e-03,  1.8882e-02,\n",
      "          7.1167e-02, -8.7288e-02, -2.9187e-02,  5.7658e-02, -2.6424e-02,\n",
      "         -8.6008e-02,  3.0803e-02,  2.9170e-02,  7.6852e-02,  7.5537e-02,\n",
      "          2.2277e-02,  7.6959e-03,  6.0045e-03,  4.6040e-02,  2.1563e-02,\n",
      "         -1.0745e-03, -4.9093e-02,  4.4913e-02, -1.9672e-02,  5.8526e-02,\n",
      "         -3.4525e-02, -8.6745e-02,  2.4846e-02, -1.5322e-02, -3.1717e-02,\n",
      "         -1.0775e-01,  1.9040e-02, -8.9190e-02,  3.9413e-02, -1.6289e-02,\n",
      "          5.7969e-02, -9.7459e-03,  7.2472e-02,  2.0903e-02, -7.0740e-02,\n",
      "         -1.0043e-01,  3.1452e-02, -1.0060e-01,  6.2442e-02,  9.0931e-02,\n",
      "         -9.8836e-02,  1.0485e-02, -5.7058e-02, -8.2929e-02,  6.1658e-02,\n",
      "         -9.5275e-02,  1.7622e-02, -2.9336e-02,  3.6360e-02, -3.3698e-02,\n",
      "         -1.4423e-02, -2.9224e-02, -1.4030e-03,  7.4905e-02,  3.4038e-03,\n",
      "         -5.1323e-03, -1.0978e-01,  1.2554e-02,  8.6753e-03,  3.0185e-02,\n",
      "         -1.3025e-02, -1.8122e-02, -3.7919e-02, -2.0929e-02,  4.4001e-02,\n",
      "         -6.6436e-02, -8.5074e-02,  7.3846e-02,  3.9703e-02, -3.3339e-02,\n",
      "          1.3987e-02, -6.0661e-02, -2.2804e-02,  1.0948e-01,  1.3480e-01,\n",
      "          9.1500e-03,  6.6593e-02,  2.4504e-02, -1.2410e-01, -1.6260e-01,\n",
      "         -6.9718e-02,  2.9161e-02,  1.8381e-02,  1.5620e-02, -3.8210e-02,\n",
      "         -3.5833e-02, -7.4793e-04,  2.0225e-02, -6.3928e-02,  2.5260e-02,\n",
      "          1.9760e-02, -3.6115e-02, -3.1435e-02, -1.0072e-02,  7.9360e-02,\n",
      "         -2.4768e-03,  7.4888e-02, -2.9773e-03,  6.4441e-02, -4.3426e-02,\n",
      "         -8.0224e-02,  4.2712e-02, -1.7489e-06, -3.5529e-02,  4.1083e-04,\n",
      "          1.0800e-02,  1.1021e-02, -4.6710e-02,  5.8155e-02, -5.6678e-02,\n",
      "          7.9008e-04, -5.3681e-02, -2.1830e-03, -2.3423e-02, -4.1740e-02,\n",
      "         -9.1049e-03,  8.9443e-02,  1.9436e-02, -1.4302e-02,  3.7254e-02,\n",
      "          1.4405e-02,  3.5515e-03,  5.0799e-02,  3.6197e-02,  1.3817e-01,\n",
      "         -9.9246e-03, -1.6555e-02, -8.0192e-02,  1.5088e-02,  2.8818e-02,\n",
      "         -5.7309e-03, -5.2207e-02,  1.5185e-02, -2.5079e-02, -4.6348e-02,\n",
      "         -2.4278e-02,  1.9144e-02,  3.0296e-02,  1.6786e-02,  9.3208e-02,\n",
      "         -3.8245e-02,  2.3377e-02,  5.8974e-03, -5.1143e-02,  1.6061e-04,\n",
      "          8.9681e-04,  6.6814e-03,  3.8083e-02,  9.5770e-02,  2.6355e-02,\n",
      "         -2.5824e-03,  3.6341e-02,  1.0706e-02, -6.2283e-03,  4.7015e-02,\n",
      "          2.3709e-02,  2.0915e-02,  6.3980e-02,  4.0833e-02,  1.9334e-02,\n",
      "          4.4466e-02,  4.1164e-02,  1.5051e-02,  4.4774e-02,  5.2658e-02,\n",
      "          5.4621e-03, -3.2650e-02,  5.1350e-02, -4.0368e-02, -1.4886e-02,\n",
      "         -4.8539e-02,  3.6036e-02,  8.9831e-03, -1.2100e-01, -3.5015e-02,\n",
      "         -2.0056e-02,  6.9250e-02, -7.0269e-03, -1.7379e-02, -2.1103e-02,\n",
      "         -2.6554e-02,  1.3877e-02, -1.4514e-02, -7.4357e-02,  4.2577e-02,\n",
      "          4.9494e-02, -2.9574e-02, -2.3533e-02, -3.0243e-04,  2.2692e-02,\n",
      "         -9.7173e-02,  3.3261e-02,  9.8141e-02,  7.4036e-02, -6.7510e-02,\n",
      "          2.3976e-02, -5.0316e-02, -1.2333e-02,  9.9901e-07, -5.6971e-02,\n",
      "          8.4983e-02, -1.1361e-01,  4.3124e-02, -2.9468e-02, -2.9050e-02,\n",
      "         -2.3498e-02, -1.3478e-02,  5.1443e-02, -2.6866e-03, -1.6774e-02,\n",
      "         -5.7375e-02,  4.6296e-02, -3.5997e-02,  2.5205e-02, -2.3101e-02,\n",
      "          8.6917e-02,  3.0726e-02, -2.9933e-02,  5.7002e-02,  1.1804e-02,\n",
      "          1.1750e-01, -5.0130e-02,  3.7178e-02,  2.2542e-03,  2.8580e-02,\n",
      "          3.9421e-02,  2.8366e-02, -2.7629e-02,  1.6794e-02,  5.0366e-03,\n",
      "         -9.0088e-02, -9.7004e-02,  1.0143e-01, -7.2282e-02, -1.7238e-02,\n",
      "          6.7603e-02, -3.0270e-02, -3.3434e-02,  4.4483e-02,  3.8962e-02,\n",
      "         -2.5167e-02, -2.1140e-02,  3.2148e-02, -1.0704e-02, -9.6192e-04,\n",
      "         -7.4437e-02, -4.2289e-02,  2.3824e-02, -1.7096e-03, -2.5320e-02,\n",
      "          2.0734e-02, -1.0219e-01, -4.0451e-02, -1.0613e-02, -8.3252e-02,\n",
      "          4.0872e-02, -1.4212e-01, -4.0303e-02, -5.9461e-02, -5.9220e-02,\n",
      "          1.1410e-02,  3.3127e-02, -7.2464e-02,  9.1532e-02, -5.1004e-02,\n",
      "         -1.1617e-02,  7.3539e-02, -2.7591e-02, -2.7198e-02,  4.1169e-02,\n",
      "         -4.0611e-02, -7.8333e-02, -7.4295e-03, -4.5057e-02, -7.5309e-02,\n",
      "         -8.5900e-03,  8.6786e-03, -2.8048e-02, -8.5403e-02, -1.3421e-02,\n",
      "         -2.8394e-02,  2.3073e-02,  7.6755e-03, -2.0844e-02, -4.5514e-02,\n",
      "          6.8001e-02, -2.5163e-02, -1.5084e-03,  8.3111e-02, -1.0045e-02,\n",
      "          2.5616e-02,  4.7857e-02,  7.0789e-02, -2.0711e-02,  2.7614e-06,\n",
      "         -7.2443e-02, -5.1617e-02, -8.5223e-02,  5.7505e-02, -3.0975e-02,\n",
      "          1.9335e-02,  2.1739e-03, -3.3905e-02, -4.0050e-02, -1.6023e-02,\n",
      "          1.8321e-02,  1.3465e-02, -1.3638e-02, -2.9385e-02,  2.9629e-02,\n",
      "          5.8468e-02, -2.7660e-02,  3.3853e-02, -2.7526e-03, -1.1211e-02,\n",
      "          1.0562e-02,  3.9206e-02, -6.0063e-02, -8.0849e-03,  2.5024e-03,\n",
      "          5.4321e-02, -4.4157e-02,  6.3013e-02,  2.6254e-02,  3.5611e-02,\n",
      "         -6.8326e-03,  6.8662e-02, -6.1535e-02, -4.8051e-03, -1.9666e-02,\n",
      "         -6.5252e-03,  6.7584e-02, -1.0928e-02,  1.0573e-01,  1.4960e-02,\n",
      "          1.0076e-02, -1.5297e-02, -1.2072e-01,  8.1879e-02, -3.3937e-02,\n",
      "         -6.4692e-02, -1.3070e-02, -1.2801e-04,  3.5698e-02, -7.1910e-02,\n",
      "          2.8157e-02, -5.4137e-03,  2.2015e-02, -4.9315e-02,  4.8521e-02,\n",
      "         -6.8749e-04,  5.6582e-02,  1.3627e-02,  2.7303e-03,  3.6877e-02,\n",
      "          6.4834e-02,  7.4802e-02,  7.1576e-02, -8.0305e-02]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('tavakolih/all-MiniLM-L6-v2-pubmed-full')\n",
    "model = AutoModel.from_pretrained('tavakolih/all-MiniLM-L6-v2-pubmed-full')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d874af59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92552bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pronto import Ontology\n",
    "import spacy\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import re\n",
    "import pandas\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def create_quantized_index(embeddings_np, d):\n",
    "    \"\"\"Create a trained IVFPQ index.\"\"\"\n",
    "    nlist = 1000\n",
    "    m = 32\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "    index.train(embeddings_np)\n",
    "    return index\n",
    "\n",
    "\n",
    "#\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched_transformers(sentences, model_name=\"tavakolih/all-MiniLM-L6-v2-pubmed-full\"):\n",
    "    \"\"\"Return average embeddings for sentences using a Transformers model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling (mean pooling function is used here)\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f96ae3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3615a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "\n",
    "path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/\"\n",
    "INPUT_FILENAME = path__+\"work/github/source_data/knowledge_base/bao/BAO.csv\"\n",
    "OUTPUT_PICKLE_FILENAME = path__+\"/work/github/CAPITAL/normalisation/dictionary/bao.pkl\"\n",
    "OUTPUT_LIST = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_list.txt\"\n",
    "FAISS_INDEX_FILENAME = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_terms.index\"\n",
    "# OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bef8e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column_content(s):\n",
    "    \"\"\"Clean and strip unwanted characters and split by pipe if present.\"\"\"\n",
    "    # First, clean the string by removing specific patterns\n",
    "    cleaned = s.strip().lower()\n",
    "    \n",
    "    # Check if the cleaned string contains a pipe symbol and split if it does\n",
    "    if '|' in cleaned:\n",
    "        return cleaned.split('|')\n",
    "    else:\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT_FILENAME, usecols=['Class ID', 'Preferred Label', 'Synonyms', 'Definitions', 'alternative term'], \n",
    "                 sep=',', engine='python', on_bad_lines='skip')\n",
    "\n",
    "\n",
    "term_to_id = {}\n",
    "embeddings = []  \n",
    "indexed_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8b69b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "for _, row in df.iterrows():\n",
    "    term_id = row['Class ID']\n",
    "    for col in ['Preferred Label', 'Synonyms', 'Definitions', 'alternative term']:\n",
    "        term_names = row[col]\n",
    "        if pd.notnull(term_names):  # Check if the term_name is not NaN\n",
    "            processed_terms = process_column_content(term_names)\n",
    "            if isinstance(processed_terms, list):\n",
    "                for term in processed_terms:\n",
    "                    flattened_data.append((term_id, term))\n",
    "            else:\n",
    "                flattened_data.append((term_id, processed_terms))\n",
    "\n",
    "# Convert flattened data to a DataFrame for easier manipulation\n",
    "flattened_df = pd.DataFrame(flattened_data, columns=['Class ID', 'Term Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0edca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor d2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2r antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>runx1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33353</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>adrenal cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>tumor of the adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33355</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant neoplasm of adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33356</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant adrenal tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33357</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>an endocrine gland cancer located_in the adren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Class ID  \\\n",
       "0       http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "4      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "...                                            ...   \n",
       "33353     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33354     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33355     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33356     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33357     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "\n",
       "                                               Term Name  \n",
       "0                  adenosine phosphodiesterase inhibitor  \n",
       "1                        dopamine receptor d2 antagonist  \n",
       "2                                         d2r antagonist  \n",
       "3                                 d2 receptor antagonist  \n",
       "4                                        runx1 inhibitor  \n",
       "...                                                  ...  \n",
       "33353                                     adrenal cancer  \n",
       "33354                         tumor of the adrenal gland  \n",
       "33355                malignant neoplasm of adrenal gland  \n",
       "33356                            malignant adrenal tumor  \n",
       "33357  an endocrine gland cancer located_in the adren...  \n",
       "\n",
       "[33358 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5213962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_df = flattened_df[0:1000]\n",
    "# flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e43cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terms: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 33358/33358 [00:01<00:00, 17559.45it/s]\n",
      "Generating Embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 334/334 [08:05<00:00,  1.45s/it]\n",
      "WARNING clustering 33342 points to 1000 centroids: please provide at least 39000 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized faiss index...\n",
      "Saving term to ID mapping and indexed terms...\n",
      "Writing terms to a txt file...\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "term_to_id = {}\n",
    "indexed_terms = []\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "term_batches = []\n",
    "id_batches = []\n",
    "current_batch_terms = []\n",
    "current_batch_ids = []\n",
    "\n",
    "for _, row in tqdm(flattened_df.iterrows(), total=flattened_df.shape[0], desc=\"Processing terms\"):\n",
    "    term_id = row['Class ID']\n",
    "    term_name = row['Term Name']\n",
    "\n",
    "    # Process the term_name\n",
    "    term_name = process_column_content(term_name)\n",
    "\n",
    "    # Check for empty or single character terms and skip them\n",
    "    if not term_name or len(term_name) <= 1:\n",
    "        continue\n",
    "\n",
    "    current_batch_terms.append(term_name)\n",
    "    current_batch_ids.append(term_id)\n",
    "\n",
    "    if len(current_batch_terms) == BATCH_SIZE:\n",
    "        term_batches.append(current_batch_terms)\n",
    "        id_batches.append(current_batch_ids)\n",
    "        current_batch_terms = []\n",
    "        current_batch_ids = []\n",
    "\n",
    "# Catch any remaining terms not added to a batch\n",
    "if current_batch_terms:\n",
    "    term_batches.append(current_batch_terms)\n",
    "    id_batches.append(current_batch_ids)\n",
    "\n",
    "for term_batch, id_batch in tqdm(zip(term_batches, id_batches), total=len(term_batches), desc=\"Generating Embeddings\"):\n",
    "    batch_embeddings = get_average_embeddings_batched_transformers(term_batch)\n",
    "    \n",
    "    for term, term_id, embedding in zip(term_batch, id_batch, batch_embeddings):\n",
    "        norm = np.linalg.norm(embedding)\n",
    "\n",
    "        # Check if the embedding is a zero vector\n",
    "        if norm == 0:\n",
    "            print(f\"Term '{term}' with ID '{term_id}' has a zero vector.\")\n",
    "\n",
    "        # Normalizing the vector\n",
    "        normalized_embedding = embedding if norm == 0 else embedding / norm\n",
    "        embeddings.append(normalized_embedding)\n",
    "        term_to_id[term] = term_id\n",
    "        indexed_terms.append(term)\n",
    "\n",
    "        # Clear out the current batch to free up memory\n",
    "    del term_batch, id_batch, batch_embeddings\n",
    "    gc.collect()\n",
    "\n",
    "# Assuming we have already calculated sentence_embeddings somewhere in the script\n",
    "d = embeddings[0].shape[0] if embeddings else 0  # Dynamically get the dimension\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "index = create_quantized_index(embeddings_np, d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Free up memory after using embeddings_np\n",
    "del embeddings, embeddings_np\n",
    "gc.collect()\n",
    "\n",
    "print(\"Saving quantized faiss index...\")\n",
    "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
    "\n",
    "# print(\"Saving term to ID mapping...\")\n",
    "# with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "#     pickle.dump(term_to_id, outfile)\n",
    "\n",
    "print(\"Saving term to ID mapping and indexed terms...\")\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "    pickle.dump({\"term_to_id\": term_to_id, \"indexed_terms\": indexed_terms}, outfile)\n",
    "\n",
    "\n",
    "print(\"Writing terms to a txt file...\")\n",
    "with open(OUTPUT_LIST, \"w\") as txt_file:\n",
    "    for term in term_to_id.keys():\n",
    "        txt_file.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93f659d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(FAISS_INDEX_FILENAME)\n",
    "\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    query = query.lower()  # Convert query to lowercase\n",
    "    query_embedding = get_average_embeddings_batched_transformers([query])[0].numpy()  # Get average embedding of the query\n",
    "\n",
    "    # Normalize the query embedding\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "\n",
    "    return similar_terms\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query_embedding = get_average_embeddings_batched_transformers([query])[0].numpy()  # Get average embedding of the query\n",
    "\n",
    "    # Normalize the query embedding\n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "\n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "\n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "\n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52aff044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: tr-fret, ID: http://www.bioassayontology.org/bao#BAO_0000004, Score: 0.3012159466743469\n",
      "Term: cret, ID: http://www.bioassayontology.org/bao#BAO_0000462, Score: 0.7369926571846008\n",
      "Term: trupath, ID: http://www.bioassayontology.org/bao#BAO_0010081, Score: 0.7620600461959839\n",
      "Term: trna, ID: http://www.bioassayontology.org/bao#BAO_0000276, Score: 0.7682545781135559\n",
      "Term: thale-cress, ID: http://purl.obolibrary.org/obo/NCBITaxon_3702, Score: 0.784970223903656\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"TR-FRET\"# \"nucleosome\"\n",
    "results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "for term, term_id, score in results:\n",
    "    print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce428e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: tr-fret, Score: 14\n",
      "Term: cin-quin, Score: 13\n",
      "Term: thale-cress, Score: 11\n",
      "Term: cret, Score: 0\n",
      "Term: trupath, Score: 0\n",
      "Term: trna, Score: 0\n",
      "Term: tcep, Score: 0\n",
      "Term: thale cress, Score: 0\n",
      "Term: gullet, Score: 0\n",
      "Term: acumen, Score: 0\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61f33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
