{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd7af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f6fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_floret_model = '/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/source_data/floret_embeddings/en_floret_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7648e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m pip install floret 'spacy~=3.4.0' pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9820c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# This is the spaCy pipeline with floret vectors\n",
    "nlp_fl = spacy.load(path_floret_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152bc246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = nlp_fl.vocab[\"sars-cov-2\"]\n",
    "word_2 = nlp_fl.vocab[\"sars-cov-2\"]\n",
    "\n",
    "word_1.similarity(word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b7df34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45980381965637207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = nlp_fl.vocab[\"sars\"]\n",
    "word_2 = nlp_fl.vocab[\"sars-cov-2\"]\n",
    "\n",
    "word_1.similarity(word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f402fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sars cov 2 [-3.5737886  2.3306377  4.0841546] 55.919779433640024\n",
      "sars [ 2.3757248  -1.0899751   0.76489997] 42.306488\n",
      "cov [-1.0599601  1.1186376  3.3393645] 57.941483\n",
      "2 [-12.03713   6.96325   8.1482 ] 152.65103\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp_fl(\"sars cov 2\")\n",
    "    \n",
    "print(tokens.text, tokens.vector[:3], tokens.vector_norm) # Only the first three components of the vector \n",
    "    \n",
    "for token in tokens:\n",
    "    print(token.text, token.vector[:3], token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fcec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pronto import Ontology\n",
    "import spacy\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(path_floret_model)\n",
    "\n",
    "\n",
    "def create_quantized_index(embeddings_np, d):\n",
    "    \"\"\"Create a trained IVFPQ index.\"\"\"\n",
    "    nlist = 1000\n",
    "    m = 30\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "    index.train(embeddings_np)\n",
    "    return index\n",
    "\n",
    "#\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0395d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "\n",
    "path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/\"\n",
    "INPUT_FILENAME = path__+\"work/github/source_data/knowledge_base/bao/BAO.csv\"\n",
    "OUTPUT_PICKLE_FILENAME = path__+\"/work/github/CAPITAL/normalisation/dictionary/bao_1.pkl\"\n",
    "OUTPUT_LIST = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_1_list.txt\"\n",
    "FAISS_INDEX_FILENAME = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_1_terms.index\"\n",
    "# OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0515907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column_content(s):\n",
    "    \"\"\"Clean and strip unwanted characters and split by pipe if present.\"\"\"\n",
    "    # First, clean the string by removing specific patterns\n",
    "    cleaned = s.strip().lower()\n",
    "    \n",
    "    # Check if the cleaned string contains a pipe symbol and split if it does\n",
    "    if '|' in cleaned:\n",
    "        return cleaned.split('|')\n",
    "    else:\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "df = pd.read_csv(INPUT_FILENAME, usecols=['Class ID', 'Preferred Label', 'Synonyms', 'Definitions', 'alternative term'], \n",
    "                 sep=',', engine='python', on_bad_lines='skip')\n",
    "\n",
    "\n",
    "term_to_id = {}\n",
    "embeddings = []  \n",
    "indexed_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01efa2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "for _, row in df.iterrows():\n",
    "    term_id = row['Class ID']\n",
    "    for col in ['Preferred Label', 'Synonyms', 'Definitions', 'alternative term']:\n",
    "        term_names = row[col]\n",
    "        if pd.notnull(term_names):  # Check if the term_name is not NaN\n",
    "            processed_terms = process_column_content(term_names)\n",
    "            if isinstance(processed_terms, list):\n",
    "                for term in processed_terms:\n",
    "                    flattened_data.append((term_id, term))\n",
    "            else:\n",
    "                flattened_data.append((term_id, processed_terms))\n",
    "\n",
    "# Convert flattened data to a DataFrame for easier manipulation\n",
    "flattened_df = pd.DataFrame(flattened_data, columns=['Class ID', 'Term Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9706cafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor d2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2r antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>d2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>runx1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33353</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>adrenal cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>tumor of the adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33355</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant neoplasm of adrenal gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33356</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>malignant adrenal tumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33357</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>an endocrine gland cancer located_in the adren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Class ID  \\\n",
       "0       http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "4      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "...                                            ...   \n",
       "33353     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33354     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33355     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33356     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "33357     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "\n",
       "                                               Term Name  \n",
       "0                  adenosine phosphodiesterase inhibitor  \n",
       "1                        dopamine receptor d2 antagonist  \n",
       "2                                         d2r antagonist  \n",
       "3                                 d2 receptor antagonist  \n",
       "4                                        runx1 inhibitor  \n",
       "...                                                  ...  \n",
       "33353                                     adrenal cancer  \n",
       "33354                         tumor of the adrenal gland  \n",
       "33355                malignant neoplasm of adrenal gland  \n",
       "33356                            malignant adrenal tumor  \n",
       "33357  an endocrine gland cancer located_in the adren...  \n",
       "\n",
       "[33358 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464d62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terms: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 33358/33358 [00:01<00:00, 18510.69it/s]\n",
      "Generating Embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:41<00:00,  1.62it/s]\n",
      "WARNING clustering 33342 points to 1000 centroids: please provide at least 39000 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized faiss index...\n",
      "Saving term to ID mapping and indexed terms...\n",
      "Writing terms to a txt file...\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "term_batches = []\n",
    "id_batches = []\n",
    "current_batch_terms = []\n",
    "current_batch_ids = []\n",
    "\n",
    "for _, row in tqdm(flattened_df.iterrows(), total=flattened_df.shape[0], desc=\"Processing terms\"):\n",
    "    term_id = row['Class ID']\n",
    "    term_name = row['Term Name']\n",
    "\n",
    "    # Assuming process_column_content is a function you've defined to process the term_name\n",
    "    term_name = process_column_content(term_name)\n",
    "\n",
    "    # Check for empty or single character terms and skip them\n",
    "    if not term_name or len(term_name) <= 1:\n",
    "        continue\n",
    "\n",
    "    current_batch_terms.append(term_name)\n",
    "    current_batch_ids.append(term_id)\n",
    "\n",
    "    if len(current_batch_terms) == BATCH_SIZE:\n",
    "        term_batches.append(current_batch_terms)\n",
    "        id_batches.append(current_batch_ids)\n",
    "        current_batch_terms = []\n",
    "        current_batch_ids = []\n",
    "\n",
    "# Catch any remaining terms not added to a batch\n",
    "if current_batch_terms:\n",
    "    term_batches.append(current_batch_terms)\n",
    "    id_batches.append(current_batch_ids)\n",
    "\n",
    "for term_batch, id_batch in tqdm(zip(term_batches, id_batches), total=len(term_batches),\n",
    "                                 desc=\"Generating Embeddings\"):\n",
    "    batch_embeddings = get_average_embeddings_batched(term_batch)\n",
    "\n",
    "    for term, term_id, embedding in zip(term_batch, id_batch, batch_embeddings):\n",
    "        norm = np.linalg.norm(embedding)\n",
    "\n",
    "        # Check if the embedding is a zero vector\n",
    "        if norm == 0:\n",
    "            print(f\"Term '{term}' with ID '{term_id}' has a zero vector.\")\n",
    "\n",
    "        # Normalizing the vector\n",
    "        normalized_embedding = embedding if norm == 0 else embedding / norm\n",
    "        embeddings.append(normalized_embedding)\n",
    "        term_to_id[term] = term_id\n",
    "        indexed_terms.append(term)\n",
    "\n",
    "        # Clear out the current batch to free up memory\n",
    "    del term_batch, id_batch, batch_embeddings\n",
    "    gc.collect()\n",
    "\n",
    "d = 300\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "index = create_quantized_index(embeddings_np, d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Free up memory after using embeddings_np\n",
    "del embeddings, embeddings_np\n",
    "gc.collect()\n",
    "\n",
    "print(\"Saving quantized faiss index...\")\n",
    "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
    "\n",
    "# print(\"Saving term to ID mapping...\")\n",
    "# with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "#     pickle.dump(term_to_id, outfile)\n",
    "\n",
    "print(\"Saving term to ID mapping and indexed terms...\")\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "    pickle.dump({\"term_to_id\": term_to_id, \"indexed_terms\": indexed_terms}, outfile)\n",
    "\n",
    "\n",
    "print(\"Writing terms to a txt file...\")\n",
    "with open(OUTPUT_LIST, \"w\") as txt_file:\n",
    "    for term in term_to_id.keys():\n",
    "        txt_file.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501e788",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f1ded3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5980432629585266, 0.605099081993103)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_embedding(term):\n",
    "    tokens = term.split()\n",
    "    # Get embeddings for each token\n",
    "    embeddings = [nlp_fl.vocab[token].vector for token in tokens if token in nlp_fl.vocab]\n",
    "    # Compute the average embedding\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "\n",
    "word_1 = nlp.vocab[\"cyclothymic disorder\"]\n",
    "word_2 = nlp.vocab[\"Cyclothymic personality\"]\n",
    "word_4 = nlp.vocab[\"Affective personality disorder\"]\n",
    "\n",
    "\n",
    "word_1.similarity(word_2), word_1.similarity(word_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcc93c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5980433\n",
      "0.69048494\n",
      "0.67023414\n"
     ]
    }
   ],
   "source": [
    "def get_average_embedding(term):\n",
    "    tokens = term.split()\n",
    "    embeddings = [nlp.vocab[token].vector for token in tokens if token in nlp.vocab]\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute cosine similarity between two vectors\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "word_1_vec = nlp.vocab[\"cyclothymic disorder\"].vector\n",
    "word_2_vec = nlp.vocab[\"Cyclothymic personality\"].vector\n",
    "word_3_vec = get_average_embedding(\"Affective personality disorder\")\n",
    "# For word_4, we get the vector of the entire phrase\n",
    "word_4_doc = nlp(\"Affective personality disorder\")\n",
    "word_4_vec = word_4_doc.vector\n",
    "\n",
    "print(cosine_similarity(word_1_vec, word_2_vec))\n",
    "print(cosine_similarity(word_1_vec, word_3_vec))\n",
    "print(cosine_similarity(word_1_vec, word_4_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3db5a18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040456e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displacement of [3H]DTG from sigma 2 receptor in rat PC12 cells incubated for 120 mins in presence of [3H]-(+)-pentazocine by liquid scintillation counting method\n",
    "# 15:34\n",
    "# Displacement of GDP-BODIPY probe from BTN3A1 full intracellular domain (unknown origin) at 0.1 mM measured after 60 mins by fluorescence polarization assay\n",
    "# 15:34\n",
    "# Binding affinity to recombinant human carbonic anhydrase 13 expressed in Escherichia coli expression system assessed as kinetic gibbs free energy change by ITC method\n",
    "# 15:34\n",
    "# Binding affinity to MDM2 in human U87MG cells assessed as inhibition of MDM2/p53 protein interaction after 10 mins by quantitative sandwich immuno assay\n",
    "\n",
    "\n",
    "# Santosh Tirunagari\n",
    "#   16:01\n",
    "# http://hl-codon-49-04.ebi.ac.uk:8888/notebooks/notebooks/normalisation%20analysis/BAO.ipynb\n",
    "\n",
    "\n",
    "# Ines Smit\n",
    "#   16:09\n",
    "# TR-FRET assay\n",
    "# 16:09\n",
    "# LC-MS analysis\n",
    "# 16:10\n",
    "# thermal shift assay\n",
    "# 16:10\n",
    "# radioligand competition binding assay\n",
    "# 16:10\n",
    "# Kinomescan method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be0fd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(path_floret_model)\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(FAISS_INDEX_FILENAME)\n",
    "\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query = query\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "    \n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "    \n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "    \n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea64105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: tr-fret, ID: http://www.bioassayontology.org/bao#BAO_0000004, Score: 0.140354186296463\n",
      "Term: presto-tango, ID: http://www.bioassayontology.org/bao#BAO_0010079, Score: 0.34693869948387146\n",
      "Term: bronsted-base, ID: http://purl.obolibrary.org/obo/CHEBI_39142, Score: 0.4002431631088257\n",
      "Term: crispr-cas9, ID: http://www.bioassayontology.org/bao#BAO_0010249, Score: 0.40232881903648376\n",
      "Term: non-linear qsar, ID: http://www.bioassayontology.org/bao#BAO_0002309, Score: 0.4076499938964844\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"TR-FRET\"# \"nucleosome\"\n",
    "results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "for term, term_id, score in results:\n",
    "    print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f6fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: hep-g2, Score: 15\n",
      "Term: oci-ly3, Score: 14\n",
      "Term: acp-tag, Score: 14\n",
      "Term: cyclin-b1, Score: 12\n",
      "Term: arrestin-gfp, Score: 11\n",
      "Term: ubiquitin-rho, Score: 10\n",
      "Term: htrf kinease-tk, Score: 9\n",
      "Term: g2/mitotic-specific cyclin-b1, Score: 6\n",
      "Term: cbf-his/runx1-biotin protein complex, Score: 5\n",
      "Term: an endocrine gland cancer located_in the adrenal glands which are located above the kidneys., Score: 0\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdf4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d66789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dde990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
