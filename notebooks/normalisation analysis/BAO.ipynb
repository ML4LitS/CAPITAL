{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd7af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f6fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_floret_model = '/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/source_data/floret_embeddings/en_floret_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7648e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m pip install floret 'spacy~=3.4.0' pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9820c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# This is the spaCy pipeline with floret vectors\n",
    "nlp_fl = spacy.load(path_floret_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152bc246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43325987458229065"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = nlp_fl.vocab[\"sars\"]\n",
    "word_2 = nlp_fl.vocab[\"sars-cove-2\"]\n",
    "\n",
    "word_1.similarity(word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f402fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sars cov 2 [-3.5737886  2.3306377  4.0841546] 55.919779433640024\n",
      "sars [ 2.3757248  -1.0899751   0.76489997] 42.306488\n",
      "cov [-1.0599601  1.1186376  3.3393645] 57.941483\n",
      "2 [-12.03713   6.96325   8.1482 ] 152.65103\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp_fl(\"sars cov 2\")\n",
    "    \n",
    "print(tokens.text, tokens.vector[:3], tokens.vector_norm) # Only the first three components of the vector \n",
    "    \n",
    "for token in tokens:\n",
    "    print(token.text, token.vector[:3], token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61e5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -pacy (/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pacy (/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages)\u001b[0m\n",
      "Requirement already satisfied: pronto in /nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages (2.5.6)\n",
      "Requirement already satisfied: networkx<4.0,>=2.3 in /nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages (from pronto) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil~=2.8 in /nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages (from pronto) (2.8.2)\n",
      "Requirement already satisfied: chardet~=5.0 in /nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages (from pronto) (5.2.0)\n",
      "Requirement already satisfied: fastobo~=0.12.2 in /nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages (from pronto) (0.12.3)\n",
      "Requirement already satisfied: six>=1.5 in /nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages (from python-dateutil~=2.8->pronto) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pacy (/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pacy (/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pacy (/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pacy (/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/nfs/production/literature/santosh_tirunagari/transformers_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fcec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pronto import Ontology\n",
    "import spacy\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import re\n",
    "import pandas\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(path_floret_model)\n",
    "\n",
    "\n",
    "def create_quantized_index(embeddings_np, d):\n",
    "    \"\"\"Create a trained IVFPQ index.\"\"\"\n",
    "    nlist = 1000\n",
    "    m = 30\n",
    "    quantizer = faiss.IndexFlatL2(d)\n",
    "    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "    index.train(embeddings_np)\n",
    "    return index\n",
    "\n",
    "#\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0395d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "\n",
    "path__ = \"/nfs/production/literature/santosh_tirunagari/BACKUP/\"\n",
    "INPUT_FILENAME = path__+\"work/github/source_data/knowledge_base/bao/BAO.csv\"\n",
    "OUTPUT_PICKLE_FILENAME = path__+\"/work/github/CAPITAL/normalisation/dictionary/bao.pkl\"\n",
    "OUTPUT_LIST = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_list.txt\"\n",
    "FAISS_INDEX_FILENAME = path__+\"work/github/CAPITAL/normalisation/dictionary/bao_terms.index\"\n",
    "# OUTPUT_INDEXED_TERMS_FILENAME = path__+\"work/github/ML_annotations/normalisation/dictionary/bao_indexed_terms.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0515907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column_content(s):\n",
    "    \"\"\"Clean and strip unwanted characters.\"\"\"\n",
    "    return re.sub(r'\\(.*?\\)|\\\".*?\\\"|\\[.*?\\]', '', s).strip().lower()\n",
    "\n",
    "\n",
    "df = pd.read_csv(input_filename, usecols=['Class ID', 'Preferred Label', 'Synonyms', 'Definitions', 'alternative term'], \n",
    "                 sep=',', engine='python', on_bad_lines='skip')\n",
    "\n",
    "\n",
    "term_to_id = {}\n",
    "embeddings = []  \n",
    "indexed_terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01efa2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ontology...\n"
     ]
    }
   ],
   "source": [
    "# Each Class ID's variations will be treated as separate entries\n",
    "print(\"Loading ontology...\")\n",
    "\n",
    "flattened_data = []\n",
    "for _, row in df.iterrows():\n",
    "    term_id = row['Class ID']\n",
    "    for col in ['Preferred Label', 'Synonyms', 'Definitions', 'alternative term']:\n",
    "        term_name = row[col]\n",
    "        if pd.notnull(term_name):  # Check if the term_name is not NaN\n",
    "            flattened_data.append((term_id, term_name))\n",
    "\n",
    "# Convert flattened data to a DataFrame for easier manipulation\n",
    "flattened_df = pd.DataFrame(flattened_data, columns=['Class ID', 'Term Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9706cafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class ID</th>\n",
       "      <th>Term Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_50444</td>\n",
       "      <td>adenosine phosphodiesterase inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>dopamine receptor D2 antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131787</td>\n",
       "      <td>D2R antagonist|D2 receptor antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>RUNX1 inhibitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_131789</td>\n",
       "      <td>acute myeloid leukemia 1 protein inhibitors|co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_77962</td>\n",
       "      <td>food antioxidant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>http://purl.obolibrary.org/obo/CHEBI_77962</td>\n",
       "      <td>food antioxidants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16052</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>adrenal gland cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16053</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>neoplasm of adrenal gland|adrenal neoplasm|adr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16054</th>\n",
       "      <td>http://purl.obolibrary.org/obo/DOID_3953</td>\n",
       "      <td>An endocrine gland cancer located_in the adren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16055 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Class ID  \\\n",
       "0       http://purl.obolibrary.org/obo/CHEBI_50444   \n",
       "1      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "2      http://purl.obolibrary.org/obo/CHEBI_131787   \n",
       "3      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "4      http://purl.obolibrary.org/obo/CHEBI_131789   \n",
       "...                                            ...   \n",
       "16050   http://purl.obolibrary.org/obo/CHEBI_77962   \n",
       "16051   http://purl.obolibrary.org/obo/CHEBI_77962   \n",
       "16052     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "16053     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "16054     http://purl.obolibrary.org/obo/DOID_3953   \n",
       "\n",
       "                                               Term Name  \n",
       "0                  adenosine phosphodiesterase inhibitor  \n",
       "1                        dopamine receptor D2 antagonist  \n",
       "2                  D2R antagonist|D2 receptor antagonist  \n",
       "3                                        RUNX1 inhibitor  \n",
       "4      acute myeloid leukemia 1 protein inhibitors|co...  \n",
       "...                                                  ...  \n",
       "16050                                   food antioxidant  \n",
       "16051                                  food antioxidants  \n",
       "16052                               adrenal gland cancer  \n",
       "16053  neoplasm of adrenal gland|adrenal neoplasm|adr...  \n",
       "16054  An endocrine gland cancer located_in the adren...  \n",
       "\n",
       "[16055 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "464d62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing terms: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16055/16055 [00:01<00:00, 14756.85it/s]\n",
      "Generating Embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [01:06<00:00,  2.41it/s]\n",
      "WARNING clustering 16043 points to 1000 centroids: please provide at least 39000 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized faiss index...\n",
      "Saving term to ID mapping and indexed terms...\n",
      "Writing terms to a txt file...\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "term_batches = []\n",
    "id_batches = []\n",
    "current_batch_terms = []\n",
    "current_batch_ids = []\n",
    "\n",
    "for _, row in tqdm(flattened_df.iterrows(), total=flattened_df.shape[0], desc=\"Processing terms\"):\n",
    "    term_id = row['Class ID']\n",
    "    term_name = row['Term Name']\n",
    "\n",
    "    # Assuming process_column_content is a function you've defined to process the term_name\n",
    "    term_name = process_column_content(term_name)\n",
    "\n",
    "    # Check for empty or single character terms and skip them\n",
    "    if not term_name or len(term_name) <= 1:\n",
    "        continue\n",
    "\n",
    "    current_batch_terms.append(term_name)\n",
    "    current_batch_ids.append(term_id)\n",
    "\n",
    "    if len(current_batch_terms) == BATCH_SIZE:\n",
    "        term_batches.append(current_batch_terms)\n",
    "        id_batches.append(current_batch_ids)\n",
    "        current_batch_terms = []\n",
    "        current_batch_ids = []\n",
    "\n",
    "# Catch any remaining terms not added to a batch\n",
    "if current_batch_terms:\n",
    "    term_batches.append(current_batch_terms)\n",
    "    id_batches.append(current_batch_ids)\n",
    "\n",
    "for term_batch, id_batch in tqdm(zip(term_batches, id_batches), total=len(term_batches),\n",
    "                                 desc=\"Generating Embeddings\"):\n",
    "    batch_embeddings = get_average_embeddings_batched(term_batch)\n",
    "\n",
    "    for term, term_id, embedding in zip(term_batch, id_batch, batch_embeddings):\n",
    "        norm = np.linalg.norm(embedding)\n",
    "\n",
    "        # Check if the embedding is a zero vector\n",
    "        if norm == 0:\n",
    "            print(f\"Term '{term}' with ID '{term_id}' has a zero vector.\")\n",
    "\n",
    "        # Normalizing the vector\n",
    "        normalized_embedding = embedding if norm == 0 else embedding / norm\n",
    "        embeddings.append(normalized_embedding)\n",
    "        term_to_id[term] = term_id\n",
    "        indexed_terms.append(term)\n",
    "\n",
    "        # Clear out the current batch to free up memory\n",
    "    del term_batch, id_batch, batch_embeddings\n",
    "    gc.collect()\n",
    "\n",
    "d = 300\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "index = create_quantized_index(embeddings_np, d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Free up memory after using embeddings_np\n",
    "del embeddings, embeddings_np\n",
    "gc.collect()\n",
    "\n",
    "print(\"Saving quantized faiss index...\")\n",
    "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
    "\n",
    "# print(\"Saving term to ID mapping...\")\n",
    "# with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "#     pickle.dump(term_to_id, outfile)\n",
    "\n",
    "print(\"Saving term to ID mapping and indexed terms...\")\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"wb\") as outfile:\n",
    "    pickle.dump({\"term_to_id\": term_to_id, \"indexed_terms\": indexed_terms}, outfile)\n",
    "\n",
    "\n",
    "print(\"Writing terms to a txt file...\")\n",
    "with open(OUTPUT_LIST, \"w\") as txt_file:\n",
    "    for term in term_to_id.keys():\n",
    "        txt_file.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b53e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855da72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0cad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40a409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1501e788",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f1ded3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5980432629585266, 0.605099081993103)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_embedding(term):\n",
    "    tokens = term.split()\n",
    "    # Get embeddings for each token\n",
    "    embeddings = [nlp_fl.vocab[token].vector for token in tokens if token in nlp_fl.vocab]\n",
    "    # Compute the average embedding\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "\n",
    "word_1 = nlp.vocab[\"cyclothymic disorder\"]\n",
    "word_2 = nlp.vocab[\"Cyclothymic personality\"]\n",
    "word_4 = nlp.vocab[\"Affective personality disorder\"]\n",
    "\n",
    "\n",
    "word_1.similarity(word_2), word_1.similarity(word_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcc93c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5980433\n",
      "0.69048494\n",
      "0.67023414\n"
     ]
    }
   ],
   "source": [
    "def get_average_embedding(term):\n",
    "    tokens = term.split()\n",
    "    embeddings = [nlp.vocab[token].vector for token in tokens if token in nlp.vocab]\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute cosine similarity between two vectors\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "word_1_vec = nlp.vocab[\"cyclothymic disorder\"].vector\n",
    "word_2_vec = nlp.vocab[\"Cyclothymic personality\"].vector\n",
    "word_3_vec = get_average_embedding(\"Affective personality disorder\")\n",
    "# For word_4, we get the vector of the entire phrase\n",
    "word_4_doc = nlp(\"Affective personality disorder\")\n",
    "word_4_vec = word_4_doc.vector\n",
    "\n",
    "print(cosine_similarity(word_1_vec, word_2_vec))\n",
    "print(cosine_similarity(word_1_vec, word_3_vec))\n",
    "print(cosine_similarity(word_1_vec, word_4_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3db5a18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fac005ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(path_floret_model)\n",
    "\n",
    "# def get_average_embedding(term):\n",
    "#     \"\"\"Get the average word embedding for a term.\"\"\"\n",
    "#     tokens = term.split()\n",
    "#     valid_vectors = [nlp.vocab[token].vector for token in tokens if\n",
    "#                      nlp.vocab[token].has_vector and nlp.vocab[token].vector.shape[0] == 300]\n",
    "\n",
    "#     if len(valid_vectors) == 0:\n",
    "#         return np.zeros((300,))\n",
    "\n",
    "#     average_embedding = np.mean(valid_vectors, axis=0)\n",
    "#     return average_embedding\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(OUTPUT_PICKLE_FILENAME, \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(FAISS_INDEX_FILENAME)\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "    \n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "    \n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "    \n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25ba8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: wgs|whole genome sequencing|wgs |whole genome sequencing, ID: http://purl.obolibrary.org/obo/NCIT_C101294, Score: 0.7766202092170715\n",
      "Term: whole genome sequencing, ID: http://purl.obolibrary.org/obo/NCIT_C101294, Score: 0.7983897924423218\n",
      "Term: treefinder phylogenetic analysis, ID: http://www.bioassayontology.org/bao#BAO_0002209, Score: 0.8390119671821594\n",
      "Term: computational phylogenetic analysis, ID: http://www.bioassayontology.org/bao#BAO_0002206, Score: 0.8426339626312256\n",
      "Term: diverset, ID: http://www.bioassayontology.org/bao#BAO_0000735, Score: 0.8775628805160522\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Kinomescan\"# \"nucleosome\"\n",
    "results = retrieve_similar_terms(query, 5)\n",
    "\n",
    "for term, term_id, score in results:\n",
    "    print(f\"Term: {term}, ID: {term_id}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25c23693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: thermal shift, Score: 96\n",
      "Term: transmittance, Score: 44\n",
      "Term: electrical current density, Score: 40\n",
      "Term: solid state laser, Score: 32\n",
      "Term: excitation filter, Score: 32\n",
      "Term: light source, Score: 31\n",
      "Term: emission wavelength, Score: 30\n",
      "Term: resonant waveguide grating, Score: 25\n",
      "Term: excitation wavelength, Score: 23\n",
      "Term: impedance, Score: 17\n"
     ]
    }
   ],
   "source": [
    "# For demonstration:\n",
    "# query = \"thermal shift \" #\"nucleosome\"\n",
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "be0fd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(path_floret_model)\n",
    "\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    \"\"\"Return average embeddings for terms.\"\"\"\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Filtering out tokens without vectors or with unexpected vector sizes\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "\n",
    "        # If no valid vectors, append a zero vector\n",
    "        if len(valid_vectors) == 0:\n",
    "            embeddings.append(np.zeros((300,)))\n",
    "        else:\n",
    "            average_embedding = np.mean(valid_vectors, axis=0)\n",
    "            embeddings.append(average_embedding)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load the term to ID mapping and indexed terms\n",
    "with open(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/uniprot_terms.pkl\", \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    term_to_id = data[\"term_to_id\"]\n",
    "    indexed_terms = data[\"indexed_terms\"]\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"/home/stirunag/work/github/ML_annotations/normalisation/dictionary/uniprot_terms.index\")\n",
    "\n",
    "def retrieve_similar_terms(query, k=5):\n",
    "    \"\"\"Retrieve top k similar terms given a query.\"\"\"\n",
    "    # Convert query to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    similar_terms = []\n",
    "    for i in range(k):\n",
    "        term = indexed_terms[I[0][i]]\n",
    "        score = D[0][i]\n",
    "        term_id = term_to_id[term]\n",
    "        similar_terms.append((term, term_id, score))\n",
    "    \n",
    "    return similar_terms\n",
    "\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy(query, k):\n",
    "    \"\"\"Retrieve k terms similar to the query.\"\"\"\n",
    "    query = query\n",
    "    \n",
    "    # Get average embedding of the query\n",
    "    query_embedding = get_average_embeddings_batched([query])\n",
    "    \n",
    "    norm = np.linalg.norm(query_embedding)\n",
    "    query_embedding = query_embedding if norm == 0 else query_embedding / norm\n",
    "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve the terms from the indexed_terms list\n",
    "    candidate_terms = [indexed_terms[i] for i in I[0]]\n",
    "    \n",
    "    # Get fuzzy matching scores for these terms\n",
    "    scores = [fuzz.ratio(query, term) for term in candidate_terms]\n",
    "    \n",
    "    # Pair up terms with their scores\n",
    "    term_score_pairs = list(zip(candidate_terms, scores))\n",
    "    \n",
    "    # Rank these pairs based on scores\n",
    "    ranked_term_score_pairs = sorted(term_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_term_score_pairs[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d8f7c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: cil-1, Score: 89\n",
      "Term: natSil-1, Score: 67\n",
      "Term: epl-1, Score: 67\n",
      "Term: blos-1, Score: 60\n",
      "Term: Msil_2912, Score: 46\n",
      "Term: Ccr1l1, Score: 40\n",
      "Term: Meg1, Score: 25\n",
      "Term: Atperox P61, Score: 13\n",
      "Term: bax, Score: 0\n",
      "Term: norpA, Score: 0\n"
     ]
    }
   ],
   "source": [
    "# For demonstration:\n",
    "query = \"IL-1\"\n",
    "results = retrieve_similar_terms_with_fuzzy(query, 10)\n",
    "for term, score in results:\n",
    "    print(f\"Term: {term}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64105e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c07536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for CD\n",
      "Loaded data for OG\n",
      "Loaded data for DS\n",
      "Loaded data for GP\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(path_floret_model)\n",
    "\n",
    "# Define mapping of annotation type to corresponding file paths\n",
    "file_mapping = {\n",
    "    'CD': ('chebi_terms.index', 'chebi_terms.pkl'),\n",
    "    'OG': ('NCBI_terms.index', 'NCBI_terms.pkl'),\n",
    "    'DS': ('umls_terms.index', 'umls_terms.pkl'),\n",
    "    'GP': ('uniprot_terms.index', 'uniprot_terms.pkl')\n",
    "}\n",
    "\n",
    "# Dictionary to hold the loaded data for each annotation type\n",
    "loaded_data = {}\n",
    "\n",
    "# Load all necessary files at the beginning\n",
    "base_path = \"/home/stirunag/work/github/CAPITAL/normalisation/dictionary/\"\n",
    "for annotation_type, (index_file, pkl_file) in file_mapping.items():\n",
    "    with open(base_path + pkl_file, \"rb\") as infile:\n",
    "        data = pickle.load(infile)\n",
    "    index = faiss.read_index(base_path + index_file)\n",
    "    loaded_data[annotation_type] = {\n",
    "        \"term_to_id\": data[\"term_to_id\"],\n",
    "        \"indexed_terms\": data[\"indexed_terms\"],\n",
    "        \"index\": index,\n",
    "        \"indexed_terms_ids\": [(term, data[\"term_to_id\"][term]) for term in data[\"indexed_terms\"]]\n",
    "    }\n",
    "    print(f\"Loaded data for {annotation_type}\")\n",
    "\n",
    "def get_average_embeddings_batched(terms):\n",
    "    docs = list(nlp.pipe(terms))\n",
    "    embeddings = []\n",
    "    for doc in docs:\n",
    "        valid_vectors = [token.vector for token in doc if token.has_vector and token.vector_norm != 0 and token.vector.shape[0] == 300]\n",
    "        embeddings.append(np.mean(valid_vectors, axis=0) if valid_vectors else np.zeros((300,)))\n",
    "    return embeddings\n",
    "\n",
    "def retrieve_similar_terms_with_fuzzy_batched(terms, annotation_type, k=3):\n",
    "    data = loaded_data[annotation_type]\n",
    "    term_to_id, indexed_terms, index, indexed_terms_ids = data[\"term_to_id\"], data[\"indexed_terms\"], data[\"index\"], data[\"indexed_terms_ids\"]\n",
    "\n",
    "    # Map transformed terms to original terms\n",
    "    original_to_transformed = {}\n",
    "    transformed_terms = []\n",
    "\n",
    "    # Check for entity groups that need transformation\n",
    "    for term in terms:\n",
    "        if annotation_type in ['CD', 'OG', 'DS']:\n",
    "            transformed_term = term.lower()\n",
    "            original_to_transformed[transformed_term] = term\n",
    "            transformed_terms.append(transformed_term)\n",
    "        else:\n",
    "            original_to_transformed[term] = term\n",
    "            transformed_terms.append(term)\n",
    "\n",
    "    term_embeddings = get_average_embeddings_batched(transformed_terms)\n",
    "    normalized_embeddings = [emb / np.linalg.norm(emb) if np.linalg.norm(emb) != 0 else emb for emb in term_embeddings]\n",
    "    D, I = index.search(np.array(normalized_embeddings).astype('float32'), k)\n",
    "\n",
    "    results = {}\n",
    "    for idx, transformed_term in enumerate(transformed_terms):\n",
    "        original_term = original_to_transformed[transformed_term]\n",
    "        candidate_terms_and_ids = [indexed_terms_ids[i] for i in I[idx]]\n",
    "        candidate_terms, candidate_ids = zip(*candidate_terms_and_ids)\n",
    "        scores = [fuzz.ratio(transformed_term, c_term.lower()) for c_term in candidate_terms]\n",
    "        results[original_term] = sorted(list(zip(candidate_terms, scores, candidate_ids)), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1a6586a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p53': [('p53', 100, 'Q8JN60'), ('Phosphoprotein p53', 29, 'P04637')],\n",
       " 'P53': [('P53', 67, 'Q42578'), ('Atperox P53', 29, 'Q42578')]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# terms = ['hypertension', 'covid-19', 'Coronavirus']\n",
    "# annotation_type = 'DS'\n",
    "\n",
    "terms = ['p53', 'P53']\n",
    "annotation_type = 'GP'\n",
    "\n",
    "\n",
    "results = retrieve_similar_terms_with_fuzzy_batched(terms, annotation_type, k=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdf4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d66789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba31fa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.22.0-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.22.0\n",
      "  Downloading Levenshtein-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.9/172.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.22.0 fuzzywuzzy-0.18.0 python-Levenshtein-0.22.0 rapidfuzz-3.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1079d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9233815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM:\n",
      "[[ 1.  2.  3.]\n",
      " [ 2.  3.  4.]\n",
      " [ 3.  4.  5.]\n",
      " [ 4.  5.  6.]\n",
      " [ 5.  6.  7.]\n",
      " [ 6.  7.  8.]\n",
      " [ 7.  8.  9.]\n",
      " [ 8.  9. 10.]]\n",
      "K: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Embed(A, L):\n",
    "    T = len(A)\n",
    "    K = T - L + 1\n",
    "    N = len(A)\n",
    "    X = np.zeros((L, K))\n",
    "    \n",
    "    for i in range(K):\n",
    "        X[:, i] = A[i:L+i]\n",
    "    \n",
    "    TM = np.hstack([X[:, 0].reshape(-1, 1), X])\n",
    "    TM = TM.T\n",
    "    \n",
    "    return TM, K\n",
    "\n",
    "# Example of use:\n",
    "A = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "L = 3\n",
    "TM, K = Embed(A, L)\n",
    "print(\"TM:\")\n",
    "print(TM[1::])\n",
    "print(\"K:\", K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b0f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "907d6a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def Hankelize(TM):\n",
    "    N, M = TM.shape\n",
    "\n",
    "    # Prepare an array to store summation and count of each diagonal\n",
    "    summation = np.zeros(N + M - 1)\n",
    "    count = np.zeros(N + M - 1)\n",
    "\n",
    "    # Iterate over the TM matrix to populate the summation and count arrays\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            summation[i + j] += TM[i, j]\n",
    "            count[i + j] += 1\n",
    "\n",
    "    # Element-wise division to get the average\n",
    "    HM = summation / count\n",
    "\n",
    "    return HM\n",
    "\n",
    "\n",
    "# Example of use:\n",
    "\n",
    "result = Hankelize(TM[1::])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dde990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
